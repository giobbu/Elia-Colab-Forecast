{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from source.utils.file_read import process_and_concat_files \n",
    "from source.utils.file_read import filter_df\n",
    "from source.utils.collect_results import collect_pb_result, collect_rmse_result, create_df_forecaster_first_stage, create_df_forecaster_second_stage\n",
    "from source.utils.generate_timestamp import generate_timestamps\n",
    "from source.forecasters.deterministic import create_day_ahead_predictions, create_day_ahead_11_predictions, create_week_ahead_predictions, create_most_recent_predictions, create_malicious_predictions, create_noisy_predictions\n",
    "from source.forecasters.probabilistic import create_day_ahead_quantiles10, create_day_ahead_11_quantiles10, create_week_ahead_quantiles10, create_most_recent_quantiles10, create_malicious_quantiles10, create_noisy_quantiles10\n",
    "from source.forecasters.probabilistic import create_day_ahead_quantiles90, create_day_ahead_11_quantiles90, create_week_ahead_quantiles90, create_most_recent_quantiles90, create_malicious_quantiles90, create_noisy_quantiles90\n",
    "from source.ensemble.stack_generalization.utils.display_results import display_forecasting_metrics\n",
    "from source.ensemble.combination_scheme.equal_weights import calculate_equal_weights\n",
    "from source.ensemble.combination_scheme.avg_weights import calculate_weighted_avg\n",
    "from source.ensemble.combination_scheme.model_selection import run_model_selection\n",
    "from source.plots.plot_forecasts import plot_ensemble_forecasts, plot_var_ensemble_forecasts, plot_weighted_avg_forecasts\n",
    "from source.plots.display_hypothesis_testing import run_statistical_comparison_analysis\n",
    "from source.plots.display_metrics import display_table_metrics\n",
    "from source.plots.display_contributions import weighted_avg_pivot_data, permutation_pivot_data, lasso_coefs_pivot_data\n",
    "from source.ensemble.combination_scheme.weight_avg_plot_importance import plot_weight_avg_contributions\n",
    "from source.assessment_contributions import compute_forecasters_contributions\n",
    "from source.ml_engine import create_ensemble_forecasts\n",
    "from sklearn.utils.fixes import parse_version, sp_version\n",
    "solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.simulation_setting import Simulation, WeightedAvg, Stack\n",
    "from source.simulation.helpers_simulation import process_combination_scheme, update_dict_weights, compute_coefficients\n",
    "from source.utils.session_ml_info import delete_previous_day_pickle\n",
    "sim_params = Simulation.testing_period\n",
    "weight_avg_params = WeightedAvg.params\n",
    "ens_params = Stack.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(sim_params['random_seed'])\n",
    "\n",
    "# process and concatenate files\n",
    "files = [sim_params['file_0'], sim_params['file_1'], sim_params['file_2'], sim_params['file_3'], sim_params['file_4']]\n",
    "df = process_and_concat_files(files)\n",
    "\n",
    "df_filtered = filter_df(df, sim_params['forecasts_col'], sim_params['measured_col'])\n",
    "\n",
    "# set buyer resource name\n",
    "buyer_resource_name = 'b1r1'\n",
    "\n",
    "# loss quantile ensemble regressor\n",
    "lst_rmse_ensemble = []\n",
    "#loss best model selection\n",
    "lst_rmse_best_model = []\n",
    "# loss equal weights scheme\n",
    "lst_rmse_equal_weights = []\n",
    "# loss weighted average scheme\n",
    "lst_rmse_weighted_avg = []\n",
    "# loss weighted average scheme soft\n",
    "lst_rmse_weighted_avg_soft = []\n",
    "# loss baseline day ahead\n",
    "lst_rmse_baseline_dayahead = []\n",
    "# loss baseline day ahead 11\n",
    "lst_rmse_baseline_dayahead11h = []\n",
    "# loss baseline week ahead\n",
    "lst_rmse_baseline_week_ahead = []\n",
    "# loss baseline most recent\n",
    "if sim_params['most_recent']:\n",
    "    lst_rmse_baseline_most_recent = []\n",
    "# loss baseline malicious\n",
    "if sim_params['malicious']:\n",
    "    lst_rmse_baseline_malicious = []\n",
    "# loss baseline noisy\n",
    "if sim_params['noisy']:\n",
    "    lst_rmse_baseline_noisy = []\n",
    "    \n",
    "# loss var ensemble regressor\n",
    "lst_rmse_var_ensemble = []\n",
    "# loss var best model selection\n",
    "lst_rmse_var_best_model = []\n",
    "# loss var equal weights scheme\n",
    "lst_rmse_var_equal_weights = []\n",
    "# loss var weighted average scheme\n",
    "lst_rmse_var_weighted_avg = []\n",
    "# loss var weighted average scheme soft\n",
    "lst_rmse_var_weighted_avg_soft = []\n",
    "# loss var baseline day ahead\n",
    "lst_rmse_var_baseline_dayahead = []\n",
    "# loss var baseline day ahead 11\n",
    "lst_rmse_var_baseline_dayahead11h = []\n",
    "# loss var baseline week ahead\n",
    "lst_rmse_var_baseline_week_ahead = []\n",
    "# loss var baseline most recent\n",
    "if sim_params['most_recent']:\n",
    "    lst_rmse_var_baseline_most_recent = []\n",
    "    lst_pb_most_recent_q10 = []\n",
    "    lst_pb_most_recent_q90 = []\n",
    "# loss var baseline malicious\n",
    "if sim_params['malicious']:\n",
    "    lst_rmse_var_baseline_malicious = []\n",
    "    lst_pb_malicious_q10 = []\n",
    "    lst_pb_malicious_q90 = []\n",
    "# loss var baseline noisy\n",
    "if sim_params['noisy']:\n",
    "    lst_rmse_var_baseline_noisy = []\n",
    "    lst_pb_noisy_q10 = []\n",
    "    lst_pb_noisy_q90 = []\n",
    "\n",
    "# loss quantile ensemble regressor\n",
    "lst_pb_ensemble_q10 = []\n",
    "lst_pb_ensemble_q90 = []\n",
    "# loss quantile best model selection\n",
    "lst_pb_best_model_q10 = []\n",
    "lst_pb_best_model_q90 = []\n",
    "# loss avg weights scheme\n",
    "lst_pb_weighted_avg_q10 = []\n",
    "lst_pb_weighted_avg_q90 = []\n",
    "# loss soft avg weights scheme\n",
    "lst_pb_weighted_avg_soft_q10 = []\n",
    "lst_pb_weighted_avg_soft_q90 = []\n",
    "# loss equal weighted scheme\n",
    "lst_pb_equal_weights_q10 = []\n",
    "lst_pb_equal_weights_q90 = []\n",
    "# loss baseline day ahead\n",
    "lst_pb_dayahead_q10 = []\n",
    "lst_pb_dayahead_q90  = []\n",
    "# loss baseline day ahead 11\n",
    "lst_pb_dayahead_11h_q10 = []\n",
    "lst_pb_dayahead_11h_q90 = []\n",
    "# loss baseline week ahead\n",
    "lst_pb_week_ahead_q10 = []\n",
    "lst_pb_week_ahead_q90 = []\n",
    "\n",
    "# remove previous day pickle file\n",
    "logger.info(' ')\n",
    "delete_previous_day_pickle()\n",
    "logger.opt(colors = True).warning('previous day pickle file removed')\n",
    "\n",
    "# final contributions forecasters\n",
    "avg_permutation_contributions = defaultdict(dict)\n",
    "avg_coefficients_contributions = defaultdict(dict)\n",
    "avg_weighted_avg_contributions = defaultdict(dict)\n",
    "avg_weighted_soft_avg_contributions = defaultdict(dict)\n",
    "\n",
    "# loop over test days\n",
    "for i in tqdm(range(sim_params['num_test_days']), desc='Testing Days'):\n",
    "\n",
    "    # generate timestamps train and prediction\n",
    "    start_training_timestamp, end_training_timestamp, start_prediction_timestamp, end_prediction_timestamp = generate_timestamps(sim_params['start_training'], i, sim_params['window_size'])\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue>-------------------------------------------------------------------------------------------</blue>')\n",
    "    logger.opt(colors=True).info(f'<blue>Start training: {start_training_timestamp} - End training: {end_training_timestamp}</blue>')\n",
    "    logger.opt(colors = True).info('<blue>-------------------------------------------------------------------------------------------</blue>')\n",
    "    logger.opt(colors = True).info(f'<blue>Start prediction: {start_prediction_timestamp} - End prediction: {end_prediction_timestamp}</blue>')\n",
    "\n",
    "    day_previous_start_prediction_timestamp = start_prediction_timestamp - pd.Timedelta('1day')\n",
    "    df_train = df_filtered[df_filtered.index.to_series().between(start_training_timestamp, end_training_timestamp)].iloc[:-1,:]\n",
    "    df_test = df_filtered[df_filtered.index.to_series().between(day_previous_start_prediction_timestamp, end_prediction_timestamp)].iloc[:-1,:]\n",
    "                                                                                                                            \n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Length of training data: {len(df_train)} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Length of test data: {len(df_test)} </blue>')\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue> -----------------> Forecasters prediction submitted </blue>')\n",
    "\n",
    "    # forecaster - day ahead forecast\n",
    "    df_day_ahead_pred_train = create_day_ahead_predictions(df_train)\n",
    "    df_day_ahead_pred_test = create_day_ahead_predictions(df_test)\n",
    "\n",
    "    # forecaster - day ahead 11 forecast\n",
    "    df_day_ahead11_pred_train = create_day_ahead_11_predictions(df_train)\n",
    "    df_day_ahead11_pred_test = create_day_ahead_11_predictions(df_test)\n",
    "\n",
    "    # forecaster - week ahead forecast\n",
    "    df_week_ahead_pred_train = create_week_ahead_predictions(df_train)\n",
    "    df_week_ahead_pred_test = create_week_ahead_predictions(df_test)\n",
    "\n",
    "    # forecaster - day ahead quantile-10\n",
    "    df_day_ahead_q10_train = create_day_ahead_quantiles10(df_train)\n",
    "    df_day_ahead_q10_test = create_day_ahead_quantiles10(df_test)\n",
    "\n",
    "    # forecaster - day ahead 11 quantile-10\n",
    "    df_day_ahead11_q10_train = create_day_ahead_11_quantiles10(df_train)\n",
    "    df_day_ahead11_q10_test = create_day_ahead_11_quantiles10(df_test)\n",
    "\n",
    "    # forecaster - week ahead quantile-10\n",
    "    df_week_ahead_q10_train = create_week_ahead_quantiles10(df_train)\n",
    "    df_week_ahead_q10_test = create_week_ahead_quantiles10(df_test)\n",
    "\n",
    "    # forecaster - day ahead quantile-90\n",
    "    df_day_ahead_q90_train = create_day_ahead_quantiles90(df_train)\n",
    "    df_day_ahead_q90_test = create_day_ahead_quantiles90(df_test)\n",
    "\n",
    "    # forecaster - day ahead 11 quantile-90\n",
    "    df_day_ahead11_q90_train = create_day_ahead_11_quantiles90(df_train)\n",
    "    df_day_ahead11_q90_test = create_day_ahead_11_quantiles90(df_test)\n",
    "\n",
    "    # forecaster - week ahead quantile-90\n",
    "    df_week_ahead_q90_train = create_week_ahead_quantiles90(df_train)\n",
    "    df_week_ahead_q90_test = create_week_ahead_quantiles90(df_test)\n",
    "\n",
    "    # forecaster - most recent forecast (intra-day market)\n",
    "    if sim_params['most_recent']:\n",
    "        # mean forecasts\n",
    "        df_most_recent_pred_train = create_most_recent_predictions(df_train)\n",
    "        df_most_recent_pred_test = create_most_recent_predictions(df_test)\n",
    "        # q10 forecasts\n",
    "        df_most_recent_q10_train = create_most_recent_quantiles10(df_train)\n",
    "        df_most_recent_q10_test = create_most_recent_quantiles10(df_test)\n",
    "        # q90 forecasts\n",
    "        df_most_recent_q90_train = create_most_recent_quantiles90(df_train)\n",
    "        df_most_recent_q90_test = create_most_recent_quantiles90(df_test)\n",
    "    \n",
    "    # forecaster - malicious forecast\n",
    "    if sim_params['malicious']:\n",
    "        # mean forecasts\n",
    "        df_malicious_pred_train = create_malicious_predictions(df_train, column= sim_params['malicious_name'])\n",
    "        df_malicious_pred_test = create_malicious_predictions(df=df_test, column= sim_params['malicious_name'], cheat=True, df_train=df_train)\n",
    "        # q10 forecasts\n",
    "        df_malicious_q10_train = create_malicious_quantiles10(df_train, column= sim_params['malicious_name'])\n",
    "        df_malicious_q10_test = create_malicious_quantiles10(df=df_test, column= sim_params['malicious_name'], cheat=True, df_train=df_train)\n",
    "        # q90 forecasts\n",
    "        df_malicious_q90_train = create_malicious_quantiles90(df_train, column= sim_params['malicious_name'])\n",
    "        df_malicious_q90_test = create_malicious_quantiles90(df=df_test, column= sim_params['malicious_name'], cheat=True, df_train=df_train)\n",
    "\n",
    "    # forecaster - noisy forecast\n",
    "    if sim_params['noisy']:\n",
    "        # mean forecasts\n",
    "        df_noisy_pred_train = create_noisy_predictions(df_train, column= sim_params['noisy_name'])\n",
    "        df_noisy_pred_test = create_noisy_predictions(df_test, column= sim_params['noisy_name'])\n",
    "        # q10 forecasts\n",
    "        df_noisy_q10_train = create_noisy_quantiles10(df_train, column= sim_params['noisy_name'])\n",
    "        df_noisy_q10_test = create_noisy_quantiles10(df_test, column= sim_params['noisy_name'])\n",
    "        # q90 forecasts\n",
    "        df_noisy_q90_train = create_noisy_quantiles90(df_train, column= sim_params['noisy_name'])\n",
    "        df_noisy_q90_test = create_noisy_quantiles90(df_test, column= sim_params['noisy_name'])\n",
    "\n",
    "# # ----------------------------> SELLERS DATA <----------------------------\n",
    "    # sellers data\n",
    "    df_train_ensemble_quantile50 = pd.concat([df_day_ahead_pred_train, df_day_ahead11_pred_train, df_week_ahead_pred_train], axis=1)\n",
    "    df_test_ensemble_quantile50 = pd.concat([df_day_ahead_pred_test, df_day_ahead11_pred_test, df_week_ahead_pred_test], axis=1)\n",
    "    if sim_params['malicious']:\n",
    "        df_train_ensemble_quantile50 = pd.concat([df_train_ensemble_quantile50, df_malicious_pred_train], axis=1)\n",
    "        df_test_ensemble_quantile50 = pd.concat([df_test_ensemble_quantile50, df_malicious_pred_test], axis=1)\n",
    "    if sim_params['most_recent']:\n",
    "        df_train_ensemble_quantile50 = pd.concat([df_train_ensemble_quantile50, df_most_recent_pred_train], axis=1)\n",
    "        df_test_ensemble_quantile50 = pd.concat([df_test_ensemble_quantile50, df_most_recent_pred_test], axis=1)\n",
    "    if sim_params['noisy']:\n",
    "        df_train_ensemble_quantile50 = pd.concat([df_train_ensemble_quantile50, df_noisy_pred_train], axis=1)\n",
    "        df_test_ensemble_quantile50 = pd.concat([df_test_ensemble_quantile50, df_noisy_pred_test], axis=1)\n",
    "    df_ensemble_quantile50 = pd.concat([df_train_ensemble_quantile50, df_test_ensemble_quantile50], axis=0)\n",
    "\n",
    "    df_train_ensemble_quantile10 = pd.concat([df_day_ahead_q10_train, df_day_ahead11_q10_train, df_week_ahead_q10_train], axis=1)\n",
    "    df_test_ensemble_quantile10 = pd.concat([df_day_ahead_q10_test, df_day_ahead11_q10_test, df_week_ahead_q10_test], axis=1)\n",
    "    if sim_params['malicious']:\n",
    "        df_train_ensemble_quantile10 = pd.concat([df_train_ensemble_quantile10, df_malicious_q10_train], axis=1)\n",
    "        df_test_ensemble_quantile10 = pd.concat([df_test_ensemble_quantile10, df_malicious_q10_test], axis=1)\n",
    "    if sim_params['most_recent']:\n",
    "        df_train_ensemble_quantile10 = pd.concat([df_train_ensemble_quantile10, df_most_recent_q10_train], axis=1)\n",
    "        df_test_ensemble_quantile10 = pd.concat([df_test_ensemble_quantile10, df_most_recent_q10_test], axis=1)\n",
    "    if sim_params['noisy']:\n",
    "        df_train_ensemble_quantile10 = pd.concat([df_train_ensemble_quantile10, df_noisy_q10_train], axis=1)\n",
    "        df_test_ensemble_quantile10 = pd.concat([df_test_ensemble_quantile10, df_noisy_q10_test], axis=1)\n",
    "    df_ensemble_quantile10 = pd.concat([df_train_ensemble_quantile10, df_test_ensemble_quantile10], axis=0)\n",
    "\n",
    "    df_train_ensemble_quantile90 = pd.concat([df_day_ahead_q90_train, df_day_ahead11_q90_train, df_week_ahead_q90_train], axis=1)\n",
    "    df_test_ensemble_quantile90 = pd.concat([df_day_ahead_q90_test, df_day_ahead11_q90_test, df_week_ahead_q90_test], axis=1)\n",
    "    if sim_params['malicious']:\n",
    "        df_train_ensemble_quantile90 = pd.concat([df_train_ensemble_quantile90, df_malicious_q90_train], axis=1)\n",
    "        df_test_ensemble_quantile90 = pd.concat([df_test_ensemble_quantile90, df_malicious_q90_test], axis=1)\n",
    "    if sim_params['most_recent']:\n",
    "        df_train_ensemble_quantile90 = pd.concat([df_train_ensemble_quantile90, df_most_recent_q90_train], axis=1)\n",
    "        df_test_ensemble_quantile90 = pd.concat([df_test_ensemble_quantile90, df_most_recent_q90_test], axis=1)\n",
    "    if sim_params['noisy']:\n",
    "        df_train_ensemble_quantile90 = pd.concat([df_train_ensemble_quantile90, df_noisy_q90_train], axis=1)\n",
    "        df_test_ensemble_quantile90 = pd.concat([df_test_ensemble_quantile90, df_noisy_q90_test], axis=1)\n",
    "\n",
    "    df_ensemble_quantile90 = pd.concat([df_train_ensemble_quantile90, df_test_ensemble_quantile90], axis=0)\n",
    "\n",
    "    lst_cols_name_q50 = ['s1_q50_b1r1', 's2_q50_b1r1', 's3_q50_b1r1']\n",
    "    lst_cols_name_q10 = ['s1_q10_b1r1', 's2_q10_b1r1', 's3_q10_b1r1']\n",
    "    lst_cols_name_q90 = ['s1_q90_b1r1', 's2_q90_b1r1', 's3_q90_b1r1']\n",
    "    \n",
    "    if sim_params['malicious']:\n",
    "        lst_cols_name_q50.append('s5_q50_b1r1')\n",
    "        lst_cols_name_q10.append('s5_q10_b1r1')\n",
    "        lst_cols_name_q90.append('s5_q90_b1r1')\n",
    "\n",
    "    if sim_params['most_recent']:\n",
    "        lst_cols_name_q50.append('s4_q50_b1r1')\n",
    "        lst_cols_name_q10.append('s4_q10_b1r1')\n",
    "        lst_cols_name_q90.append('s4_q90_b1r1')\n",
    "\n",
    "    if sim_params['noisy']:\n",
    "        lst_cols_name_q50.append('s6_q50_b1r1')\n",
    "        lst_cols_name_q10.append('s6_q10_b1r1')\n",
    "        lst_cols_name_q90.append('s6_q90_b1r1')\n",
    "\n",
    "    df_ensemble_quantile50.columns = lst_cols_name_q50\n",
    "    df_ensemble_quantile10.columns = lst_cols_name_q10\n",
    "    df_ensemble_quantile90.columns = lst_cols_name_q90\n",
    "\n",
    "    df_market = pd.concat([df_ensemble_quantile50, df_ensemble_quantile10, df_ensemble_quantile90], axis=1)  \n",
    "\n",
    "# # ----------------------------> BUYERS DATA <----------------------------\n",
    "    df_train_buyer = pd.DataFrame(df_train['measured'])\n",
    "    df_test_buyer = pd.DataFrame(df_test['measured'])\n",
    "    forecast_range = pd.date_range(start=start_prediction_timestamp, end=end_prediction_timestamp, freq='15min')\n",
    "    df_test_buyer['measured'] = [None for i in range(len(df_test_buyer))]\n",
    "    df_buyer = pd.concat([df_train_buyer, df_test_buyer], axis=0)\n",
    "    df_buyer['b1r1'] = df_buyer['measured']\n",
    "    df_buyer.drop(columns=['measured'], inplace=True)\n",
    "\n",
    "# # ----------------------------> PREDICO PLATFORM ML ENGINE <----------------------------\n",
    "    results_ensemble_forecasts = create_ensemble_forecasts(ens_params=ens_params,\n",
    "                                                            df_buyer=df_buyer, \n",
    "                                                            df_market=df_market,\n",
    "                                                            end_training_timestamp=end_training_timestamp,\n",
    "                                                            forecast_range = forecast_range,\n",
    "                                                            challenge_usecase='simulation',\n",
    "                                                            simulation=True)\n",
    "\n",
    "# # ----------------------------> COMBINATION SCHEME DATA <----------------------------\n",
    "    if sim_params['malicious']:\n",
    "        # mean forecasts\n",
    "        df_train['maliciousforecast'] = df_malicious_pred_train.values\n",
    "        df_test['maliciousforecast'] = df_malicious_pred_test.values\n",
    "        # q10 forecasts\n",
    "        df_train['maliciousconfidence10'] = df_malicious_q10_train.values\n",
    "        df_test['maliciousconfidence10'] = df_malicious_q10_test.values\n",
    "        # q90 forecasts\n",
    "        df_train['maliciousconfidence90'] = df_malicious_q90_train.values\n",
    "        df_test['maliciousconfidence90'] = df_malicious_q90_test.values\n",
    "\n",
    "    if sim_params['noisy']:\n",
    "        # mean forecasts\n",
    "        df_train['noisyforecast'] = df_noisy_pred_train.values\n",
    "        df_test['noisyforecast'] = df_noisy_pred_test.values\n",
    "        # q10 forecasts\n",
    "        df_train['noisyconfidence10'] = df_noisy_q10_train.values\n",
    "        df_test['noisyconfidence10'] = df_noisy_q10_test.values\n",
    "        # q90 forecasts\n",
    "        df_train['noisyconfidence90'] = df_noisy_q90_train.values\n",
    "        df_test['noisyconfidence90'] = df_noisy_q90_test.values\n",
    "\n",
    "    df_train_norm, day_previous_df_test_norm, day_previous_df_test_norm_var = process_combination_scheme(df_train, df_test, end_training_timestamp, day_previous_start_prediction_timestamp)\n",
    "    df_pred_ensemble = results_ensemble_forecasts['wind_power']['predictions']   \n",
    "    df_pred_ensemble.rename(columns={'q50_' + 'b1r1': '50_predictions', 'q10_' + 'b1r1': '10_predictions', 'q90_' + 'b1r1': '90_predictions', 'norm_' + 'b1r1': 'target'}, inplace=True)\n",
    "    df_pred_ensemble['target'] = day_previous_df_test_norm['norm_measured'].values[-96:]\n",
    "    df_var_ensemble = results_ensemble_forecasts['wind_power_ramp']['predictions']\n",
    "    df_var_ensemble.rename(columns={'q50_' + 'b1r1': '50_var_predictions', 'q10_' + 'b1r1': '10_var_predictions', 'q90_' + 'b1r1': '90_var_predictions', 'targets': 'target'}, inplace=True)\n",
    "    df_var_ensemble['target'] = day_previous_df_test_norm_var['norm_measured'].values[-96:]\n",
    "    \n",
    "    df_test_ensemble = pd.DataFrame(df_pred_ensemble['target']) \n",
    "    df_2stage_test = pd.DataFrame(df_var_ensemble['target'])\n",
    "\n",
    "# # ----------------------------> PERFORMANCE METRICS <----------------------------\n",
    "    # performance ensemble\n",
    "    lst_rmse_ensemble, rmse_ensemble = collect_rmse_result(df_pred_ensemble, '50_predictions', lst_rmse_ensemble)\n",
    "\n",
    "    lst_pb_ensemble_q10, lst_pb_ensemble_q90, pinball_ensemble_q10, pinball_ensemble_q90 = collect_pb_result(df_pred_ensemble, \n",
    "                                                                                                            '10_predictions', '90_predictions', \n",
    "                                                                                                            lst_pb_ensemble_q10, lst_pb_ensemble_q90)\n",
    "\n",
    "    # performance variability ensemble\n",
    "    lst_rmse_var_ensemble, rmse_var_ensemble = collect_rmse_result(df_var_ensemble, '50_var_predictions', lst_rmse_var_ensemble)\n",
    "\n",
    "    # performance best model selection\n",
    "    df_best_model_var = run_model_selection(sim_params, df_train_norm , day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp , window_size_valid = weight_avg_params['window_size_valid'], var=True)\n",
    "    lst_rmse_var_best_model, rmse_var_best_model = collect_rmse_result(df_best_model_var, 'mean_prediction', lst_rmse_var_best_model)\n",
    "\n",
    "    # performance weighted average\n",
    "    df_weighted_avg_var, dict_weights_var = calculate_weighted_avg(sim_params, df_train_norm , day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp , window_size_valid=weight_avg_params['window_size_valid'], var=True)\n",
    "    lst_rmse_var_weighted_avg, rmse_var_weighted_avg = collect_rmse_result(df_weighted_avg_var, 'mean_prediction', lst_rmse_var_weighted_avg)\n",
    "\n",
    "    # performance weighted avg soft\n",
    "    df_weighted_avg_soft_var, dict_weights_soft_var = calculate_weighted_avg(sim_params, df_train_norm, day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'], var=True, norm='softmax')\n",
    "    lst_rmse_var_weighted_avg_soft, rmse_var_weighted_avg_soft = collect_rmse_result(df_weighted_avg_soft_var, 'mean_prediction', lst_rmse_var_weighted_avg_soft)\n",
    "\n",
    "    # plot contribution weighted average\n",
    "    if ens_params['plot_importance_weighted_avg']:\n",
    "        plot_weight_avg_contributions(dict_weights_var, quantile=0.5, stage='Wind Power Variability', days= weight_avg_params['window_size_valid'])\n",
    "\n",
    "    # performance equal weights\n",
    "    df_equal_weights_var = calculate_equal_weights(day_previous_df_test_norm_var, start_prediction_timestamp)\n",
    "    lst_rmse_var_equal_weights, rmse_var_equal_weights = collect_rmse_result(df_equal_weights_var, 'mean_prediction', lst_rmse_var_equal_weights)\n",
    "\n",
    "    # performance day-ahead\n",
    "    df_dayahead_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'dayahead', start_prediction_timestamp)\n",
    "    lst_rmse_var_baseline_dayahead, rmse_var_dayahead = collect_rmse_result(df_dayahead_var, 'norm_dayaheadforecast', lst_rmse_var_baseline_dayahead)\n",
    "\n",
    "    # performance day-ahead-11h\n",
    "    df_dayahead_11h_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'dayahead11h', start_prediction_timestamp)\n",
    "    lst_rmse_var_baseline_dayahead11h, rmse_var_dayahead_11h = collect_rmse_result(df_dayahead_11h_var, 'norm_dayahead11hforecast', lst_rmse_var_baseline_dayahead11h)\n",
    "\n",
    "    # performance week ahead\n",
    "    df_week_ahead_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'weekahead', start_prediction_timestamp)\n",
    "    lst_rmse_var_baseline_week_ahead, rmse_var_week_ahead = collect_rmse_result(df_week_ahead_var, 'norm_weekaheadforecast', lst_rmse_var_baseline_week_ahead)\n",
    "\n",
    "    # performance most recent\n",
    "    if sim_params['most_recent']:\n",
    "        df_most_recent_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'mostrecent', start_prediction_timestamp)\n",
    "        lst_rmse_var_baseline_most_recent, rmse_var_most_recent = collect_rmse_result(df_most_recent_var, 'norm_mostrecentforecast', lst_rmse_var_baseline_most_recent)\n",
    "\n",
    "    # performance malicious\n",
    "    if sim_params['malicious']:\n",
    "        df_malicious_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'malicious', start_prediction_timestamp)\n",
    "        lst_rmse_var_baseline_malicious, rmse_var_malicious = collect_rmse_result(df_malicious_var, 'norm_maliciousforecast', lst_rmse_var_baseline_malicious)\n",
    "\n",
    "    # performance noisy\n",
    "    if sim_params['noisy']:\n",
    "        df_noisy_var = create_df_forecaster_second_stage(day_previous_df_test_norm_var, 'noisy', start_prediction_timestamp)\n",
    "        lst_rmse_var_baseline_noisy, rmse_var_noisy = collect_rmse_result(df_noisy_var, 'norm_noisyforecast', lst_rmse_var_baseline_noisy)\n",
    "\n",
    "    # performance best model selection\n",
    "    df_best_model = run_model_selection(sim_params, df_train_norm, day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'])\n",
    "    lst_rmse_best_model, rmse_best_model = collect_rmse_result(df_best_model, 'mean_prediction', lst_rmse_best_model)\n",
    "    lst_pb_best_model_q10, lst_pb_best_model_q90, pinball_best_model_q10, pinball_best_model_q90 = collect_pb_result(df_best_model,\n",
    "                                                                                                                    'Q10', 'Q90',\n",
    "                                                                                                                    lst_pb_best_model_q10, lst_pb_best_model_q90)\n",
    "\n",
    "    # performance weighted average\n",
    "    df_weighted_avg, dict_weights = calculate_weighted_avg(sim_params, df_train_norm, day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'])\n",
    "    lst_rmse_weighted_avg, rmse_weighted_avg = collect_rmse_result(df_weighted_avg, 'mean_prediction', lst_rmse_weighted_avg)\n",
    "    lst_pb_weighted_avg_q10, lst_pb_weighted_avg_q90, pinball_weighted_avg_q10, pinball_weighted_avg_q90 = collect_pb_result(df_weighted_avg, \n",
    "                                                                                                                                'Q10', 'Q90', \n",
    "                                                                                                                                lst_pb_weighted_avg_q10, lst_pb_weighted_avg_q90)\n",
    "    # performance weighted avg soft\n",
    "    df_weighted_avg_soft, dict_weights_soft = calculate_weighted_avg(sim_params, df_train_norm, day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'], norm='softmax')\n",
    "    lst_rmse_weighted_avg_soft, rmse_weighted_avg_soft = collect_rmse_result(df_weighted_avg_soft, 'mean_prediction', lst_rmse_weighted_avg_soft)\n",
    "    lst_pb_weighted_avg_soft_q10, lst_pb_weighted_avg_soft_q90, pinball_weighted_avg_soft_q10, pinball_weighted_avg_soft_q90 = collect_pb_result(df_weighted_avg_soft,\n",
    "                                                                                                                                                'Q10', 'Q90', \n",
    "                                                                                                                                                lst_pb_weighted_avg_soft_q10, lst_pb_weighted_avg_soft_q90)\n",
    "    # plot forecasts weighted avg\n",
    "    if ens_params['plot_weighted_avg']:\n",
    "        plot_weighted_avg_forecasts(df_weighted_avg)\n",
    "\n",
    "    # plot contribution weighted average\n",
    "    if ens_params['plot_importance_weighted_avg']:\n",
    "        for quantile in ens_params['quantiles']:\n",
    "            plot_weight_avg_contributions(dict_weights, quantile, stage='Wind Power', days = weight_avg_params['window_size_valid'])\n",
    "\n",
    "    # performance equal weights\n",
    "    df_equal_weights = calculate_equal_weights(day_previous_df_test_norm, start_prediction_timestamp)\n",
    "    lst_rmse_equal_weights, rmse_equal_weights = collect_rmse_result(df_equal_weights, 'mean_prediction', lst_rmse_equal_weights)\n",
    "    lst_pb_equal_weights_q10, lst_pb_equal_weights_q90, pinball_equal_weights_q10, pinball_equal_weights_q90 = collect_pb_result(df_equal_weights, \n",
    "                                                                                                                                        'Q10', 'Q90', \n",
    "                                                                                                                                        lst_pb_equal_weights_q10, lst_pb_equal_weights_q90)\n",
    "    # performance day-ahead\n",
    "    df_dayahead = create_df_forecaster_first_stage(day_previous_df_test_norm, 'dayahead', start_prediction_timestamp)\n",
    "    lst_rmse_baseline_dayahead, rmse_dayahead = collect_rmse_result(df_dayahead, 'norm_dayaheadforecast', lst_rmse_baseline_dayahead)\n",
    "    lst_pb_dayahead_q10, lst_pb_dayahead_q90, pinball_dayahead_q10, pinball_dayahead_q90 = collect_pb_result(df_dayahead, \n",
    "                                                                                                                'norm_dayaheadconfidence10', 'norm_dayaheadconfidence90', \n",
    "                                                                                                                lst_pb_dayahead_q10, lst_pb_dayahead_q90)\n",
    "    # performance day-ahead-11h\n",
    "    df_dayahead_11h = create_df_forecaster_first_stage(day_previous_df_test_norm, 'dayahead11h', start_prediction_timestamp)\n",
    "    lst_rmse_baseline_dayahead11h, rmse_dayahead_11h = collect_rmse_result(df_dayahead_11h, 'norm_dayahead11hforecast', lst_rmse_baseline_dayahead11h)\n",
    "    lst_pb_dayahead_11h_q10, lst_pb_dayahead_11h_q90, pinball_dayahead_11h_q10, pinball_dayahead_11h_q90 = collect_pb_result(df_dayahead_11h, \n",
    "                                                                                                                                'norm_dayahead11hconfidence10', 'norm_dayahead11hconfidence90', \n",
    "                                                                                                                                lst_pb_dayahead_11h_q10, lst_pb_dayahead_11h_q90)\n",
    "    # performance week ahead\n",
    "    df_week_ahead = create_df_forecaster_first_stage(day_previous_df_test_norm, 'weekahead', start_prediction_timestamp)\n",
    "    lst_rmse_baseline_week_ahead, rmse_week_ahead = collect_rmse_result(df_week_ahead, 'norm_weekaheadforecast', lst_rmse_baseline_week_ahead)\n",
    "    lst_pb_week_ahead_q10, lst_pb_week_ahead_q90, pinball_week_ahead_q10, pinball_week_ahead_q90 = collect_pb_result(df_week_ahead, \n",
    "                                                                                                                        'norm_weekaheadconfidence10', 'norm_weekaheadconfidence90', \n",
    "                                                                                                                        lst_pb_week_ahead_q10, lst_pb_week_ahead_q90) \n",
    "    # performance most recent\n",
    "    if sim_params['most_recent']:\n",
    "        df_most_recent = create_df_forecaster_first_stage(day_previous_df_test_norm, 'mostrecent', start_prediction_timestamp)\n",
    "        lst_rmse_baseline_most_recent, rmse_most_recent = collect_rmse_result(df_most_recent, 'norm_mostrecentforecast', lst_rmse_baseline_most_recent)\n",
    "        lst_pb_most_recent_q10, lst_pb_most_recent_q90, pinball_most_recent_q10, pinball_most_recent_q90 = collect_pb_result(df_most_recent, \n",
    "                                                                                                                                'norm_mostrecentconfidence10', 'norm_mostrecentconfidence90', \n",
    "                                                                                                                                lst_pb_most_recent_q10, lst_pb_most_recent_q90) \n",
    "    # performance malicious cheat\n",
    "    if sim_params['malicious']:\n",
    "        df_malicious = create_df_forecaster_first_stage(day_previous_df_test_norm, 'malicious', start_prediction_timestamp)\n",
    "        lst_rmse_baseline_malicious, rmse_malicious = collect_rmse_result(df_malicious, 'norm_maliciousforecast', lst_rmse_baseline_malicious)\n",
    "        lst_pb_malicious_q10, lst_pb_malicious_q90, pinball_malicious_q10, pinball_malicious_q90 = collect_pb_result(df_malicious, \n",
    "                                                                                                                        'norm_maliciousconfidence10', 'norm_maliciousconfidence90', \n",
    "                                                                                                                        lst_pb_malicious_q10, lst_pb_malicious_q90)\n",
    "    # performance noisy\n",
    "    if sim_params['noisy']:\n",
    "        df_noisy = create_df_forecaster_first_stage(day_previous_df_test_norm, 'noisy', start_prediction_timestamp)\n",
    "        lst_rmse_baseline_noisy, rmse_noisy = collect_rmse_result(df_noisy, 'norm_noisyforecast', lst_rmse_baseline_noisy)\n",
    "        lst_pb_noisy_q10, lst_pb_noisy_q90, pinball_noisy_q10, pinball_noisy_q90 = collect_pb_result(df_noisy, \n",
    "                                                                                                        'norm_noisyconfidence10', 'norm_noisyconfidence90', \n",
    "                                                                                                        lst_pb_noisy_q10, lst_pb_noisy_q90)\n",
    "    # plot forecasts\n",
    "    if ens_params['plt_wind_power_ensemble']:\n",
    "        plot_ensemble_forecasts(df_pred_ensemble, df_test_ensemble)\n",
    "        nr_previous_days = len(pd.date_range(start=start_training_timestamp, end=end_training_timestamp, freq='1D')) - 1\n",
    "        plt.title(f'Ensemble Forecasts - Quantile {ens_params[\"model_type\"]}')\n",
    "        #plot_ramp_events(df_test_norm_diff, ens_params['compute_abs_difference'])\n",
    "        if not ens_params['zoom_in_variability']:  # zoom in the variability forecasts\n",
    "            plt.ylim(-0.01, 1)\n",
    "        plt.show()\n",
    "        \n",
    "    # plot variability forecast results\n",
    "    if ens_params['plt_wind_power_variability_ensemble']:\n",
    "        plot_var_ensemble_forecasts(df_var_ensemble, df_2stage_test)\n",
    "        nr_previous_days = len(pd.date_range(start=start_training_timestamp, end=end_training_timestamp, freq='1D')) - 1\n",
    "        plt.title(f'Ensemble Variability Forecasts - Quantile {ens_params[\"var_model_type\"]}')\n",
    "        #plot_ramp_events(df_test_norm_diff, ens_params['compute_abs_difference'])\n",
    "        if not ens_params['zoom_in_variability']:  # zoom in the variability forecasts\n",
    "            plt.ylim(-0.6, 0.6)\n",
    "        plt.show()\n",
    "\n",
    "    ## ----------------------------> DISPLAY METRICS <----------------------------\n",
    "    if sim_params['display_metrics']:\n",
    "        results_metrics = {'ensemble': {'rmse': rmse_ensemble, \n",
    "                                        'pb10': pinball_ensemble_q10, \n",
    "                                        'pb90': pinball_ensemble_q90, \n",
    "                                        'rmse_var': rmse_var_ensemble},\n",
    "                            'best_model': {'rmse': rmse_best_model,\n",
    "                                            'pb10': pinball_best_model_q10, \n",
    "                                            'pb90': pinball_best_model_q90, \n",
    "                                            'rmse_var': rmse_var_best_model},\n",
    "                            'weighted_avg': {'rmse': rmse_weighted_avg, \n",
    "                                            'pb10': pinball_weighted_avg_q10, \n",
    "                                            'pb90': pinball_weighted_avg_q90, \n",
    "                                            'rmse_var': rmse_var_weighted_avg},\n",
    "                            'weighted_avg_soft': {'rmse': rmse_weighted_avg_soft, \n",
    "                                                'pb10': pinball_weighted_avg_soft_q10, \n",
    "                                                'pb90': pinball_weighted_avg_soft_q90, \n",
    "                                                'rmse_var': rmse_var_weighted_avg_soft},\n",
    "                            'equal_weights': {'rmse': rmse_equal_weights, \n",
    "                                            'pb10': pinball_equal_weights_q10, \n",
    "                                            'pb90': pinball_equal_weights_q90, \n",
    "                                            'rmse_var': rmse_var_equal_weights},\n",
    "                            'day_ahead': {'rmse': rmse_dayahead, \n",
    "                                        'pb10': pinball_dayahead_q10, \n",
    "                                        'pb90': pinball_dayahead_q90, \n",
    "                                        'rmse_var': rmse_var_dayahead},\n",
    "                            'day_ahead_11h': {'rmse': rmse_dayahead_11h, \n",
    "                                            'pb10': pinball_dayahead_11h_q10, \n",
    "                                            'pb90': pinball_dayahead_11h_q90, \n",
    "                                            'rmse_var': rmse_var_dayahead_11h},\n",
    "                            'week_ahead': {'rmse': rmse_week_ahead, \n",
    "                                        'pb10': pinball_week_ahead_q10, \n",
    "                                        'pb90': pinball_week_ahead_q90, \n",
    "                                        'rmse_var': rmse_var_week_ahead}\n",
    "                            }\n",
    "        if sim_params['most_recent']:\n",
    "            results_metrics['most_recent'] = {'rmse': rmse_most_recent, \n",
    "                                            'pb10': pinball_most_recent_q10, \n",
    "                                            'pb90': pinball_most_recent_q90, \n",
    "                                            'rmse_var': rmse_var_most_recent}\n",
    "        if sim_params['malicious']:\n",
    "            results_metrics['malicious'] = {'rmse': rmse_malicious, \n",
    "                                                    'pb10': pinball_malicious_q10, \n",
    "                                                    'pb90': pinball_malicious_q90, \n",
    "                                                    'rmse_var': rmse_var_malicious}\n",
    "        if sim_params['noisy']:\n",
    "            results_metrics['noisy'] = {'rmse': rmse_noisy, \n",
    "                                            'pb10': pinball_noisy_q10, \n",
    "                                            'pb90': pinball_noisy_q90, \n",
    "                                            'rmse_var': rmse_var_noisy}\n",
    "        \n",
    "        display_forecasting_metrics(sim_params=sim_params, ens_params=ens_params, dict_metrics = results_metrics)\n",
    "    \n",
    "    # # ----------------------------> FORECASTERS PERMUTATION CONTRIBUTIONS <----------------------------\n",
    "    y_test = df_test['measured'].values[-96:]\n",
    "    iter_permutation_contributions = compute_forecasters_contributions(buyer_resource_name, ens_params, y_test, forecast_range)\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue> -----------------> Forecasters permutation contributions computed </blue>')\n",
    "    avg_permutation_contributions = update_dict_weights(avg_permutation_contributions, iter_permutation_contributions, iteration=i)\n",
    "\n",
    "    # # ----------------------------> FORECASTERS COEFFICIENTS CONTRIBUTIONS <----------------------------\n",
    "    if ens_params['model_type'] == 'LR':\n",
    "        logger.info(' ')\n",
    "        logger.opt(colors = True).info('<blue> -----------------> Forecasters coefficients contributions computed </blue>')\n",
    "        with open('/Users/gio/Desktop/Elia-RES-Forecasting/info_model/b1r1_previous_day.pickle', 'rb') as handle:\n",
    "            previous_day = pickle.load(handle)\n",
    "        iter_coefficients_contributions = compute_coefficients(previous_day)\n",
    "        avg_coefficients_contributions = update_dict_weights(avg_coefficients_contributions, \n",
    "                                                            iter_coefficients_contributions, \n",
    "                                                            iteration=i)\n",
    "\n",
    "    # # ----------------------------> FORECASTERS WEIGHTED AVERAGE CONTRIBUTIONS (SUM NORMALIZATION) <---------------------------- \n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue> -----------------> Forecasters weighted average contributions computed </blue>')\n",
    "    iter_weighted_avg_contributions = defaultdict(dict)\n",
    "    iter_weighted_avg_contributions['wind_power'] = dict_weights\n",
    "    iter_weighted_avg_contributions['wind_power_ramp'] = dict_weights_var\n",
    "    avg_weighted_avg_contributions = update_dict_weights(avg_weighted_avg_contributions, iter_weighted_avg_contributions, iteration=i)\n",
    "\n",
    "    ## ----------------------------> FORECASTERS WEIGHTED AVERAGE CONTRIBUTIONS (SOFTMAX NORMALIZATION) <----------------------------\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue> -----------------> Forecasters weighted average contributions (softmax) computed </blue>')\n",
    "    iter_weighted_avg_soft_contributions = defaultdict(dict)\n",
    "    iter_weighted_avg_soft_contributions['wind_power'] = dict_weights_soft\n",
    "    iter_weighted_avg_soft_contributions['wind_power_ramp'] = dict_weights_soft_var\n",
    "    avg_weighted_soft_avg_contributions = update_dict_weights(avg_weighted_soft_avg_contributions, iter_weighted_avg_soft_contributions, iteration=i)\n",
    "\n",
    "    # Clear output\n",
    "    #clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most recent\n",
    "if not sim_params['most_recent']:\n",
    "    lst_rmse_baseline_most_recent = None\n",
    "    lst_pb_most_recent_q10 = None\n",
    "    lst_pb_most_recent_q90 = None\n",
    "    lst_rmse_var_baseline_most_recent = None\n",
    "    \n",
    "# malicious\n",
    "if not sim_params['malicious']:\n",
    "    lst_rmse_baseline_malicious = None\n",
    "    lst_pb_malicious_q10 = None\n",
    "    lst_pb_malicious_q90 = None\n",
    "    lst_rmse_var_baseline_malicious = None\n",
    "\n",
    "# noisy\n",
    "if not sim_params['noisy']:\n",
    "    lst_rmse_baseline_noisy = None\n",
    "    lst_pb_noisy_q10 = None\n",
    "    lst_pb_noisy_q90 = None\n",
    "    lst_rmse_var_baseline_noisy = None\n",
    "\n",
    "# plot statistical comparison q50\n",
    "title1='RMSE-based Statistical Significance'\n",
    "title2='RMSE-based Statistical Comparison: critical difference diagram of ranks'\n",
    "data_q50, avg_rank_q50 = run_statistical_comparison_analysis(ens_params['model_type'],\n",
    "                                                                lst_rmse_ensemble,\n",
    "                                                                lst_rmse_best_model,\n",
    "                                                                lst_rmse_equal_weights, \n",
    "                                                                lst_rmse_weighted_avg,\n",
    "                                                                lst_rmse_weighted_avg_soft,\n",
    "                                                                lst_rmse_baseline_dayahead, \n",
    "                                                                lst_rmse_baseline_dayahead11h, \n",
    "                                                                lst_rmse_baseline_week_ahead,\n",
    "                                                                lst_rmse_baseline_most_recent,\n",
    "                                                                lst_rmse_baseline_malicious,\n",
    "                                                                lst_rmse_baseline_noisy,\n",
    "                                                                title1, title2)\n",
    "# plot statistical comparison q10\n",
    "title1 = 'Q10 Pinball loss-based Statistical Significance'\n",
    "title2 = 'Q10 Pinball loss-based Statistical Comparison: critical difference diagram of ranks'\n",
    "data_q10, avg_rank_q10 = run_statistical_comparison_analysis(ens_params['model_type'],\n",
    "                                                            lst_pb_ensemble_q10, \n",
    "                                                            lst_pb_best_model_q10,\n",
    "                                                            lst_pb_equal_weights_q10, \n",
    "                                                            lst_pb_weighted_avg_q10, \n",
    "                                                            lst_pb_weighted_avg_soft_q10,\n",
    "                                                            lst_pb_dayahead_q10, \n",
    "                                                            lst_pb_dayahead_11h_q10, \n",
    "                                                            lst_pb_week_ahead_q10,\n",
    "                                                            lst_pb_most_recent_q10,\n",
    "                                                            lst_pb_malicious_q10,\n",
    "                                                            lst_pb_noisy_q10,\n",
    "                                                            title1, title2)\n",
    "# plot statistical comparison q90\n",
    "title1 = 'Q90 Pinball loss-based Statistical Significance'\n",
    "title2 = 'Q90 Pinball loss-based Statistical Comparison: critical difference diagram of ranks'\n",
    "data_q90, avg_rank_q90 = run_statistical_comparison_analysis(ens_params['model_type'],\n",
    "                                                            lst_pb_ensemble_q90,\n",
    "                                                            lst_pb_best_model_q90,\n",
    "                                                            lst_pb_equal_weights_q90, \n",
    "                                                            lst_pb_weighted_avg_q90,\n",
    "                                                            lst_pb_weighted_avg_soft_q90, \n",
    "                                                            lst_pb_dayahead_q90, \n",
    "                                                            lst_pb_dayahead_11h_q90, \n",
    "                                                            lst_pb_week_ahead_q90,\n",
    "                                                            lst_pb_most_recent_q90,\n",
    "                                                            lst_pb_malicious_q90,\n",
    "                                                            lst_pb_noisy_q90,\n",
    "                                                            title1, title2)\n",
    "# plot statistical comparison variability\n",
    "title1 = 'RMSE-based Statistical Significance'\n",
    "title2 = 'RMSE-based Statistical Comparison: critical difference diagram of ranks'\n",
    "data_q50_var, avg_rank_q50_var = run_statistical_comparison_analysis(ens_params['var_model_type'],\n",
    "                                                                lst_rmse_var_ensemble,\n",
    "                                                                lst_rmse_var_best_model, \n",
    "                                                                lst_rmse_var_equal_weights, \n",
    "                                                                lst_rmse_var_weighted_avg,\n",
    "                                                                lst_rmse_var_weighted_avg_soft, \n",
    "                                                                lst_rmse_var_baseline_dayahead, \n",
    "                                                                lst_rmse_var_baseline_dayahead11h, \n",
    "                                                                lst_rmse_var_baseline_week_ahead,\n",
    "                                                                lst_rmse_var_baseline_most_recent,\n",
    "                                                                lst_rmse_var_baseline_malicious,\n",
    "                                                                lst_rmse_var_baseline_noisy,\n",
    "                                                                title1, title2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate random data for the histogram\n",
    "most_recent = np.array(lst_rmse_baseline_most_recent)\n",
    "ensemble = np.array(lst_rmse_ensemble)\n",
    "equal_weights = np.array(lst_rmse_equal_weights)\n",
    "\n",
    "# Plotting a basic histogram\n",
    "plt.hist(most_recent, bins=15, color='skyblue')\n",
    "plt.hist(ensemble, bins=15, color='orange')\n",
    "plt.hist(equal_weights, bins=15, color='green')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Values')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Basic Histogram')\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the styled DataFrame\n",
    "dfs = [data_q10, data_q50, data_q90, data_q50_var]\n",
    "prefixes = ['Q10', 'Q50', 'Q90', 'Q50_var']\n",
    "result, styled_result = display_table_metrics(dfs, prefixes)\n",
    "styled_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute percentage of improvement of the best\n",
    "perc_improvement_df = (result/np.min(result, axis=0)-1)*100\n",
    "perc_improvement_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = weighted_avg_pivot_data(sim_params, avg_weighted_avg_contributions)\n",
    "\n",
    "# # Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Chart of Average Weighted Avg Scheme Contributions')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title('Heatmap of Average Weighted Avg Scheme Contributions')\n",
    "plt.show()\n",
    "\n",
    "if sim_params['save_scenario_contributions']:\n",
    "    type_score = 'avg_scheme'\n",
    "    scenario = sim_params['scenario']\n",
    "    df_pivot.to_csv(f'/Users/gio/Desktop/Elia-RES-Forecasting/info_model/df_pivot_{scenario}_{type_score}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = weighted_avg_pivot_data(sim_params, avg_weighted_soft_avg_contributions)\n",
    "\n",
    "# # Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Chart of Average Weighted Soft Avg Scheme Contributions')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title('Heatmap of Average Weighted Soft Avg Scheme Contributions')\n",
    "plt.show()\n",
    "\n",
    "if sim_params['save_scenario_contributions']:\n",
    "    type_score = 'avg_scheme'\n",
    "    scenario = sim_params['scenario']\n",
    "    df_pivot.to_csv(f'/Users/gio/Desktop/Elia-RES-Forecasting/info_model/df_pivot_{scenario}_{type_score}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = permutation_pivot_data(sim_params, avg_permutation_contributions)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Chart of Average Permutation Contributions')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title('Heatmap of Average Permutation Contributions')\n",
    "plt.show()\n",
    "\n",
    "if sim_params['save_scenario_contributions']:\n",
    "    type_score = 'permutation'\n",
    "    scenario = sim_params['scenario']\n",
    "    df_pivot.to_csv(f'/Users/gio/Desktop/Elia-RES-Forecasting/info_model/df_pivot_{scenario}_{type_score}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = lasso_coefs_pivot_data(sim_params, avg_coefficients_contributions)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title(\"Stacked Bar Chart of Average Lasso's Coefficients Contributions\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title(\"Heatmap of Average Lasso's Coefficients Contributions\")\n",
    "plt.show()\n",
    "\n",
    "if sim_params['save_scenario_contributions']:\n",
    "    type_score = 'coefs'\n",
    "    scenario = sim_params['scenario']\n",
    "    df_pivot.to_csv(f'/Users/gio/Desktop/Elia-RES-Forecasting/info_model/df_pivot_{scenario}_{type_score}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_coefficients_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_coefficients_contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_train[['mostrecentforecast', 'dayaheadforecast', 'dayahead11hforecast', 'weekaheadforecast']].corr()\n",
    "correlation_matrix.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_num = np.array([13, 5, 9, 34, 2])\n",
    "print('offline', np.mean(all_num))\n",
    "print('-----------------')\n",
    "for i, a in enumerate(all_num):\n",
    "    n = i+1\n",
    "    if n == 1:\n",
    "        mu = a\n",
    "    else:\n",
    "        mu = mu + (a - mu)/n\n",
    "    print('a:', a, 'n:', n, 'mu:', mu)\n",
    "print('online', mu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
