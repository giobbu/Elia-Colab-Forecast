{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "sys.path.append(os.getenv(\"PATH_CURRENT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from sklearn.metrics import mean_pinball_loss\n",
    "from source.utils.file_read import process_and_concat_files \n",
    "from source.utils.file_read import filter_df\n",
    "from source.simulation.submission_module import submission_forecasters\n",
    "from source.simulation.buyer_module import prepare_buyer_data\n",
    "from source.utils.generate_timestamp import generate_timestamps\n",
    "from source.ml_engine import create_ensemble_forecasts\n",
    "from source.simulation.helpers_simulation import process_combination_scheme\n",
    "from source.ensemble.stack_generalization.ramp_detection.boxplot_detector import detect_wind_ramp_boxplot\n",
    "from source.ensemble.stack_generalization.ramp_detection.lof_detector import detect_wind_ramp_lof\n",
    "from source.ensemble.stack_generalization.ramp_detection.kde_detector import detect_wind_ramp_kde\n",
    "from source.ensemble.stack_generalization.ramp_detection.eq_detector import detect_wind_ramp_eq\n",
    "from source.ensemble.stack_generalization.ramp_detection.utils import process_ramp_events\n",
    "\n",
    "from source.plots.plot_forecasts import plot_forecasts, plot_var_forecasts, plot_ramp_detection\n",
    "from sklearn.utils.fixes import parse_version, sp_version\n",
    "solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.ramp_calib_setting import Simulation, WeightedAvg, Stack\n",
    "from source.utils.session_ml_info import delete_previous_day_pickle\n",
    "sim_params = Simulation.testing_period\n",
    "weight_avg_params = WeightedAvg.params\n",
    "ens_params = Stack.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wind_ramp_events_day(df, list_ramp_alarm, i):\n",
    "    \"\"\"\n",
    "    Checks for wind ramp events in a specific day and returns their occurrences.\n",
    "    \"\"\"\n",
    "    # Get the datetime from the ramp alarm list\n",
    "    datetime = list_ramp_alarm[i][0]\n",
    "    # Process ramp events and get the updated dataframe and threshold\n",
    "    df, _ = process_ramp_events(df)\n",
    "    # Filter the dataframe for measurements within the specific day\n",
    "    df_day_measurements = df.loc[datetime : datetime + pd.Timedelta(days=1)]\n",
    "    # Check if there are any wind ramp events for the day\n",
    "    wind_ramp = df_day_measurements['ramp_events'].sum() > 0\n",
    "    # If wind ramp events exist, get their indices\n",
    "    list_wind_ramps = []\n",
    "    if wind_ramp:\n",
    "        list_wind_ramps = df_day_measurements[df_day_measurements['ramp_events'] == 1].index.tolist()\n",
    "    return list_wind_ramps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_stacked_bar_chart(df_pivot, title):\n",
    "    \"\"\"\n",
    "    Plots a stacked bar chart using the provided pivot DataFrame.\n",
    "    \"\"\"\n",
    "    # Plot the stacked bar chart\n",
    "    df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "    plt.xlabel('Key and Quantile')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f\"Stacked Bar Chart of {title} Contributions\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_heatmap(df_pivot, title):\n",
    "    \"\"\"\n",
    "    Plots a heatmap using the provided pivot DataFrame.\n",
    "    \"\"\"\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.heatmap(df_pivot, annot=True, cmap='viridis')\n",
    "    plt.xlabel('Series')\n",
    "    plt.ylabel('Key and Quantile')\n",
    "    plt.title(f\"Heatmap of {title} Contributions\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(sim_params['random_seed'])\n",
    "\n",
    "# process and concatenate files\n",
    "files = [sim_params['file_1'], sim_params['file_2'], sim_params['file_3'], sim_params['file_4'], \n",
    "            sim_params['file_5'], sim_params['file_6'], sim_params['file_7'], sim_params['file_8'], \n",
    "            sim_params['file_9'], sim_params['file_10'], sim_params['file_11'], sim_params['file_12']]\n",
    "logger.info(' ')\n",
    "logger.info(f'Load Files: {files}')\n",
    "\n",
    "df = process_and_concat_files(files)\n",
    "\n",
    "# filter data forecasters\n",
    "df_filtered = filter_df(df, sim_params['forecasts_col'], sim_params['measured_col'])\n",
    "\n",
    "# replace NaN values\n",
    "if sim_params['replace_nan']:\n",
    "    logger.info(' ')\n",
    "    logger.warning(\"Replacing NaN values with 0s\")\n",
    "    print(df_filtered.isna().sum())\n",
    "    df_filtered.fillna(0, inplace=True)\n",
    "\n",
    "# set buyer resource name\n",
    "buyer_resource_name = 'b1r1'\n",
    "\n",
    "# remove previous day pickle file\n",
    "logger.info(' ')\n",
    "delete_previous_day_pickle()\n",
    "logger.opt(colors = True).warning('previous day pickle file removed')\n",
    "\n",
    "# Collect Ramp Alarm\n",
    "list_ramp_alarm = []\n",
    "# Collect Intraday Ramp Alarm\n",
    "list_ramp_alarm_intraday = []\n",
    "\n",
    "from collections import defaultdict\n",
    "from source.simulation.helpers_simulation import update_dict_weights\n",
    "avg_permutation_contributions = defaultdict(dict)\n",
    "avg_shapley_contributions = defaultdict(dict)\n",
    "\n",
    "\n",
    "# loop over test days\n",
    "for i in tqdm(range(sim_params['num_test_days']), desc='Testing Days'):\n",
    "\n",
    "    # generate timestamps train and prediction\n",
    "    start_training_timestamp, end_training_timestamp, start_prediction_timestamp, end_prediction_timestamp = generate_timestamps(sim_params['start_training'], i, sim_params['window_size'])\n",
    "\n",
    "    if i >= 5:\n",
    "        day_calibration = 5\n",
    "        start_training_timestamp = start_training_timestamp - pd.Timedelta('5day')\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue>-------------------------------------------------------------------------------------------</blue>')\n",
    "    logger.opt(colors=True).info(f'<blue>Start training: {start_training_timestamp} - End training: {end_training_timestamp}</blue>')\n",
    "    logger.opt(colors = True).info('<blue>-------------------------------------------------------------------------------------------</blue>')\n",
    "    logger.opt(colors = True).info(f'<blue>Start prediction: {start_prediction_timestamp} - End prediction: {end_prediction_timestamp}</blue>')\n",
    "\n",
    "    day_previous_start_prediction_timestamp = start_prediction_timestamp #- pd.Timedelta('1day')\n",
    "    df_train = df_filtered[df_filtered.index.to_series().between(start_training_timestamp, end_training_timestamp)].iloc[:-1,:]\n",
    "    df_test = df_filtered[df_filtered.index.to_series().between(day_previous_start_prediction_timestamp, end_prediction_timestamp)].iloc[:-1,:]\n",
    "                                                                                                                            \n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Length of training data: {len(df_train)} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Length of test data: {len(df_test)} </blue>')\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info('<blue> -----------------> Forecasters prediction submitted </blue>')\n",
    "\n",
    "# # ----------------------------> FORECASTERS PREDICTION SUBMISSION <----------------------------\n",
    "\n",
    "    df_market, df_train, df_test = submission_forecasters(sim_params, df_train, df_test)  \n",
    "\n",
    "# # ----------------------------> target DATA <----------------------------\n",
    "\n",
    "    df_buyer, forecast_range = prepare_buyer_data(df_train, df_test, start_prediction_timestamp, end_prediction_timestamp)\n",
    "\n",
    "# # ----------------------------> PREDICO PLATFORM ML ENGINE <----------------------------\n",
    "\n",
    "# # ----------------------------> ENSEMBLE FORECASTS <----------------------------\n",
    "\n",
    "    # #------------------------------------ case 1)\n",
    "\n",
    "    # df_market.drop(df_market.filter(like='q10').columns, axis=1, inplace=True)\n",
    "    # df_market.drop(df_market.filter(like='q90').columns, axis=1, inplace=True)\n",
    "\n",
    "    # #------------------------------------ case 2) \n",
    "\n",
    "    # # randomly raplace float values with nans in df_market\n",
    "    # df_market = df_market.mask(np.random.random(df_market.shape) < 0.1)\n",
    "\n",
    "    # #------------------------------------ case 3) \n",
    "    # import pickle\n",
    "    # with open('forecast_data.pickle', 'rb') as f:\n",
    "    #     data = pickle.load(f)\n",
    "    # launch_time = data['launch_time']\n",
    "    # import pytz\n",
    "    # utc = pytz.UTC \n",
    "    # forecast_range = data['forecast_range']\n",
    "    # end_training_timestamp = pd.to_datetime(min(launch_time.replace(tzinfo=utc), forecast_range[0].replace(tzinfo=utc)), utc=True)\n",
    "    # df_market_train = data['df_market'].bfill().iloc[:-192]\n",
    "    # df_market_test = data['df_market'].iloc[-96:]\n",
    "    # # concatenate\n",
    "    # df_market = pd.concat([df_market_train, df_market_test])\n",
    "    # df_buyer_train = data['df_buyer'].iloc[:-192]\n",
    "    # df_buyer_test = data['df_buyer'].iloc[-96:]\n",
    "    # # concatenate\n",
    "    # df_buyer = pd.concat([df_buyer_train, df_buyer_test])\n",
    "    # challenge_usecase = data['challenge_usecase']\n",
    "    # challenge_id = data['challenge_id']\n",
    "\n",
    "    results_predico_forecasts = create_ensemble_forecasts(ens_params=ens_params,\n",
    "                                                            df_buyer=df_buyer, \n",
    "                                                            df_market=df_market,\n",
    "                                                            end_training_timestamp=end_training_timestamp,\n",
    "                                                            forecast_range = forecast_range,\n",
    "                                                            challenge_usecase='simulation',\n",
    "                                                            simulation=True)\n",
    "    \n",
    "    \n",
    "    # # ----------------------------> TARGET VARIABILITY<----------------------------\n",
    "    df_train_norm, day_previous_df_test_norm, day_previous_df_test_norm_var = process_combination_scheme(df_train, df_test, end_training_timestamp, day_previous_start_prediction_timestamp)\n",
    "\n",
    "    df_pred_plot = results_predico_forecasts['wind_power']['predictions']\n",
    "    df_pred_plot.rename(columns={'q50_' + 'b1r1': '50_predictions', 'q10_' + 'b1r1': '10_predictions', 'q90_' + 'b1r1': '90_predictions'}, inplace=True)\n",
    "    df_test_plot = pd.DataFrame(day_previous_df_test_norm['norm_measured'].iloc[-96:])\n",
    "    df_test_plot.columns = ['targets']\n",
    "\n",
    "    # drop 'norm_measured' column\n",
    "    df_train_norm = df_train_norm.drop(columns=['norm_measured'])\n",
    "    day_previous_df_test_norm = day_previous_df_test_norm.drop(columns=['norm_measured'])\n",
    "\n",
    "    # get last 96 values of the day_previous_df_test_norm\n",
    "    df_test_norm = day_previous_df_test_norm.iloc[-96:]\n",
    "    target_variability = day_previous_df_test_norm_var['norm_measured'].values[-96:]\n",
    "\n",
    "    # Predictions Insample and Outsample retrieved from the ensemble forecasts\n",
    "    pred_var_insample = results_predico_forecasts['wind_power_variability']['predictions_insample']\n",
    "    pred_var_outsample = results_predico_forecasts['wind_power_variability']['predictions_outsample']\n",
    "\n",
    "    # set params for ramp detection\n",
    "    preprocess_ramps = ens_params['preprocess_ramps']\n",
    "    max_consecutive_points = ens_params['max_consecutive_points']\n",
    "\n",
    "    # Wind Ramp Detection using Boxplot technique\n",
    "    if ens_params['detector'] == 'box':\n",
    "        list_ramp_alarm, alarm_status, upper_box_bound, df_ramp_clusters = detect_wind_ramp_boxplot(pred_insample = pred_var_insample, \n",
    "                                                                                                    pred_outsample = pred_var_outsample, \n",
    "                                                                                                    forecast_range=forecast_range, \n",
    "                                                                                                    list_ramp_alarm = list_ramp_alarm, \n",
    "                                                                                                    df_train = df_train, \n",
    "                                                                                                    q1 = ens_params['q1_box'], \n",
    "                                                                                                    q3 = ens_params['q3_box'], \n",
    "                                                                                                    k = ens_params['k_box'], \n",
    "                                                                                                    preprocess_ramps = preprocess_ramps,\n",
    "                                                                                                    max_consecutive_points = max_consecutive_points)\n",
    "    # Wind Ramp Detection using KDE technique\n",
    "    if ens_params['detector'] == 'kde':\n",
    "        list_ramp_alarm, alarm_status, df_ramp_clusters = detect_wind_ramp_kde(df_train = df_train,\n",
    "                                                                                df_insample = pred_var_insample, \n",
    "                                                                                df_outsample = pred_var_outsample, \n",
    "                                                                                forecast_range=forecast_range, \n",
    "                                                                                list_ramp_alarm = list_ramp_alarm, \n",
    "                                                                                threshold_quantile = ens_params['threshold_quantile_kde'],\n",
    "                                                                                preprocess_ramps = preprocess_ramps,\n",
    "                                                                                cv_folds = ens_params['cv_folds_kde'],\n",
    "                                                                                max_consecutive_points = max_consecutive_points)\n",
    "    # Wind Ramp Detection using EQ technique\n",
    "    if ens_params['detector'] == 'eq':\n",
    "        list_ramp_alarm, list_ramp_alarm_intraday, alarm_status, df_ramp_clusters = detect_wind_ramp_eq(df_train = df_train,\n",
    "                                                                                                        df_insample = pred_var_insample, \n",
    "                                                                                                        df_outsample = pred_var_outsample,\n",
    "                                                                                                        list_ramp_alarm = list_ramp_alarm, \n",
    "                                                                                                        threshold_quantile = ens_params['threshold_quantile_eq'],\n",
    "                                                                                                        list_ramp_alarm_intraday = list_ramp_alarm_intraday,\n",
    "                                                                                                        preprocess_ramps = preprocess_ramps,\n",
    "                                                                                                        max_consecutive_points = max_consecutive_points)\n",
    "    # Wind Ramp Detection using LOF technique\n",
    "    if ens_params['detector'] == 'lof':\n",
    "        list_ramp_alarm, alarm_status, df_ramp_clusters = detect_wind_ramp_lof(pred_insample = pred_var_insample, \n",
    "                                                                                    pred_outsample = pred_var_outsample,\n",
    "                                                                                    df_train_norm = df_train_norm, \n",
    "                                                                                    df_test_norm = df_test_norm, \n",
    "                                                                                    forecast_range=forecast_range, \n",
    "                                                                                    list_ramp_alarm = list_ramp_alarm, \n",
    "                                                                                    df_train = df_train, \n",
    "                                                                                    n_neighbors = ens_params['n_neighbors_lof'], \n",
    "                                                                                    contamination = ens_params['contamination_lof'],\n",
    "                                                                                    preprocess_ramps = preprocess_ramps,\n",
    "                                                                                    max_consecutive_points = max_consecutive_points)\n",
    "\n",
    "\n",
    "    # # # # ----------------------------> PLOT FORECASTS <----------------------------\n",
    "\n",
    "    # compute rmse\n",
    "    rmse = np.sqrt(np.mean((df_pred_plot['50_predictions'].values - df_test_plot['targets'].values)**2))\n",
    "    # pinball loss q10\n",
    "    pinball_loss_q10 = mean_pinball_loss(df_test_plot['targets'].values, df_pred_plot['10_predictions'].values, alpha=0.1)\n",
    "    # pinball loss q90\n",
    "    pinball_loss_q90 = mean_pinball_loss(df_test_plot['targets'].values, df_pred_plot['90_predictions'].values, alpha=0.9)\n",
    "\n",
    "    df_pred_var_plot = results_predico_forecasts['wind_power_variability']['predictions'][['q50_b1r1', 'q10_b1r1', 'q90_b1r1']]\n",
    "    df_pred_var_plot.rename(columns={'q50_' + 'b1r1': '50_var_predictions', 'q10_' + 'b1r1': '10_var_predictions', 'q90_' + 'b1r1': '90_var_predictions'}, inplace=True)\n",
    "    df_test_var_plot = pd.DataFrame(day_previous_df_test_norm_var['norm_measured'].iloc[-96:])\n",
    "    df_test_var_plot.columns = ['targets']\n",
    "\n",
    "    # # # # ----------------------------> WIND RAMP EVENTS <----------------------------\n",
    "    list_wind_ramps = check_wind_ramp_events_day(df, list_ramp_alarm, i)\n",
    "\n",
    "    str_forecaster = 'dayahead'\n",
    "    df_dayahead = df_test.filter(like=str_forecaster, axis=1)\n",
    "    if str_forecaster == 'dayahead':\n",
    "        df_dayahead.drop(['dayahead11hforecast', 'dayahead11hconfidence10','dayahead11hconfidence90'], axis=1, inplace=True)\n",
    "    # replace with \"50_predictions\", \"10_predictions\", \"90_predictions\"\n",
    "    df_dayahead.columns = ['50_predictions', '10_predictions', '90_predictions']\n",
    "    # retain last 96 values\n",
    "    df_dayahead = df_dayahead.iloc[-96:]\n",
    "\n",
    "    # compute rmse\n",
    "    rmse_dayahead = np.sqrt(np.mean((df_dayahead['50_predictions'].values - df_test_plot['targets'].values)**2))\n",
    "    # pinball loss q10\n",
    "    pinball_loss_q10_dayahead = mean_pinball_loss(df_test_plot['targets'].values, df_dayahead['10_predictions'].values, alpha=0.1)\n",
    "    # pinball loss q90\n",
    "    pinball_loss_q90_dayahead = mean_pinball_loss(df_test_plot['targets'].values, df_dayahead['90_predictions'].values, alpha=0.9)\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters: wind power ensemble</blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> RMSE: {rmse} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q10: {pinball_loss_q10} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q90: {pinball_loss_q90} </blue>')\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters: dayahead </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> RMSE: {rmse_dayahead} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q10: {pinball_loss_q10_dayahead} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q90: {pinball_loss_q90_dayahead} </blue>')\n",
    "\n",
    "    # filter dataframes where \"mostrecent\" is contained in name\n",
    "    df_dayahead11 = df_test.filter(like='dayahead11h', axis=1)\n",
    "    # replace with \"50_predictions\", \"10_predictions\", \"90_predictions\"\n",
    "    df_dayahead11.columns = ['50_predictions', '10_predictions', '90_predictions']\n",
    "    # retain last 96 values\n",
    "    df_dayahead11 = df_dayahead11.iloc[-96:]\n",
    "\n",
    "    # compute rmse\n",
    "    rmse_dayahead11 = np.sqrt(np.mean((df_dayahead11['50_predictions'].values - df_test_plot['targets'].values)**2))\n",
    "    # pinball loss q10\n",
    "    pinball_loss_q10_dayahead11 = mean_pinball_loss(df_test_plot['targets'].values, df_dayahead11['10_predictions'].values, alpha=0.1)\n",
    "    # pinball loss q90\n",
    "    pinball_loss_q90_dayahead11 = mean_pinball_loss(df_test_plot['targets'].values, df_dayahead11['90_predictions'].values, alpha=0.9)\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters: dayahead11h </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> RMSE: {rmse_dayahead11} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q10: {pinball_loss_q10_dayahead11} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q90: {pinball_loss_q90_dayahead11} </blue>')\n",
    "\n",
    "    # filter dataframes where \"weekahead\" is contained in name\n",
    "    df_weekahead = df_test.filter(like='weekahead', axis=1)\n",
    "    # replace with \"50_predictions\", \"10_predictions\", \"90_predictions\"\n",
    "    df_weekahead.columns = ['50_predictions', '10_predictions', '90_predictions']\n",
    "    # retain last 96 values\n",
    "    df_weekahead = df_weekahead.iloc[-96:]\n",
    "\n",
    "    # compute rmse\n",
    "    rmse_weekahead = np.sqrt(np.mean((df_weekahead['50_predictions'].values - df_test_plot['targets'].values)**2))\n",
    "    # pinball loss q10\n",
    "    pinball_loss_q10_weekahead = mean_pinball_loss(df_test_plot['targets'].values, df_weekahead['10_predictions'].values, alpha=0.1)\n",
    "    # pinball loss q90\n",
    "    pinball_loss_q90_weekahead = mean_pinball_loss(df_test_plot['targets'].values, df_weekahead['90_predictions'].values, alpha=0.9)\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters: weekahead </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> RMSE: {rmse_weekahead} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q10: {pinball_loss_q10_weekahead} </blue>')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q90: {pinball_loss_q90_weekahead} </blue>')\n",
    "\n",
    "    if sim_params['most_recent']:\n",
    "        # filter dataframes where \"mostrecent\" is contained in name\n",
    "        df_most_recent = df_test.filter(like='mostrecent', axis=1)\n",
    "        # replace with \"50_predictions\", \"10_predictions\", \"90_predictions\"\n",
    "        df_most_recent.columns = ['50_predictions', '10_predictions', '90_predictions']\n",
    "        # retain last 96 values\n",
    "        df_most_recent = df_most_recent.iloc[-96:]\n",
    "\n",
    "        # compute rmse\n",
    "        rmse_most_recent = np.sqrt(np.mean((df_most_recent['50_predictions'].values - df_test_plot['targets'].values)**2))\n",
    "        # pinball loss q10\n",
    "        pinball_loss_q10_most_recent = mean_pinball_loss(df_test_plot['targets'].values, df_most_recent['10_predictions'].values, alpha=0.1)\n",
    "        # pinball loss q90\n",
    "        pinball_loss_q90_most_recent = mean_pinball_loss(df_test_plot['targets'].values, df_most_recent['90_predictions'].values, alpha=0.9)\n",
    "\n",
    "        logger.info(' ')\n",
    "        logger.opt(colors = True).info(f'<blue> -----------------> Forecasters: mostrecent </blue>')\n",
    "        logger.opt(colors = True).info(f'<blue> -----------------> RMSE: {rmse_most_recent} </blue>')\n",
    "        logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q10: {pinball_loss_q10_most_recent} </blue>')\n",
    "        logger.opt(colors = True).info(f'<blue> -----------------> Pinball Loss Q90: {pinball_loss_q90_most_recent} </blue>')\n",
    "\n",
    "    # collect rmse results in list\n",
    "    list_rmse = [rmse_dayahead, rmse_dayahead11, rmse_weekahead]\n",
    "    # collect Q10 pinball loss results in list\n",
    "    list_pinball_loss_q10 = [pinball_loss_q10_dayahead, pinball_loss_q10_dayahead11, pinball_loss_q10_weekahead]  \n",
    "    # collect Q90 pinball loss results in list\n",
    "    list_pinball_loss_q90 = [pinball_loss_q90_dayahead, pinball_loss_q90_dayahead11, pinball_loss_q90_weekahead]\n",
    "    if sim_params['most_recent']:\n",
    "        list_rmse.append(rmse_most_recent)\n",
    "        list_pinball_loss_q10.append(pinball_loss_q10_most_recent)\n",
    "        list_pinball_loss_q90.append(pinball_loss_q90_most_recent)\n",
    "\n",
    "    # plot as stacked bar chart\n",
    "    df_results = pd.DataFrame({'1/(Pinball Loss Q10)': list_pinball_loss_q10, '1/RMSE': list_rmse, '1/(Pinball Loss Q90)': list_pinball_loss_q90})\n",
    "    list_names = ['dayahead', 'dayahead11h', 'weekahead']\n",
    "    if sim_params['most_recent']:\n",
    "        list_names.append('mostrecent')\n",
    "    df_results.index = list_names\n",
    "    # compute 1/loss for each metric\n",
    "    df_accuracy = 1/df_results\n",
    "    # normalize by sum\n",
    "    df_accuracy = df_accuracy.div(df_accuracy.sum(axis=0), axis=1).T\n",
    "\n",
    "    # plot stack bar chart with x-axis the metric and y-axis the accuracy and as color the forecasters\n",
    "    df_accuracy.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Stacked Bar Chart of Forecasters Accuracy')\n",
    "\n",
    "    # plot heatmap\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.heatmap(df_accuracy.T, annot=True, cmap='viridis')\n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('FAccuracy')\n",
    "    plt.title('Heatmap of Forecasters Accuracy')\n",
    "\n",
    "    # plot day-ahead forecasts\n",
    "    plot_forecasts(df_dayahead, df_test_plot, list_wind_ramps, title = 'Wind Power - Day-Ahead Forecasts', color='blue')\n",
    "\n",
    "    # plot day-ahead-11h forecasts\n",
    "    plot_forecasts(df_dayahead11, df_test_plot, list_wind_ramps, title = 'Wind Power - Day-Ahead-11h Forecasts', color='orange')\n",
    "\n",
    "    # plot week-ahead forecasts\n",
    "    plot_forecasts(df_weekahead, df_test_plot, list_wind_ramps, title = 'Wind Power - Week-Ahead Forecasts', color='green')\n",
    "\n",
    "    # plot most recent forecasts\n",
    "    if sim_params['most_recent']:\n",
    "        plot_forecasts(df_most_recent, df_test_plot, list_wind_ramps, title = 'Wind Power - Most-Recent Forecasts', color='gray')\n",
    "\n",
    "    # plot wind power forecast results\n",
    "    if ens_params['plt_wind_power_ensemble']:\n",
    "        plot_forecasts(df_pred_plot, df_test_plot, list_wind_ramps, title = 'Wind Power - QR Forecasts')\n",
    "\n",
    "    # plot variability forecast results\n",
    "    if ens_params['plt_wind_power_variability_ensemble']:\n",
    "        plot_var_forecasts(df_pred_var_plot, df_test_var_plot, list_wind_ramps, title = 'Wind Power Variability - QR Forecasts')\n",
    "\n",
    "    if not df_ramp_clusters.empty:\n",
    "        num_ramp_cluster_events = len(df_ramp_clusters.cluster_id.unique())\n",
    "        logger.info(' ')\n",
    "        logger.opt(colors = True).info(f'<blue> -----------------> Number of Ramp Cluster Events: {num_ramp_cluster_events} </blue>')\n",
    "        plot_ramp_detection(df_test_var_plot, df_pred_var_plot, df_ramp_clusters, list_wind_ramps)\n",
    "\n",
    "    # # # # ----------------------------> FORECASTERS PERMUTATION CONTRIBUTIONS <----------------------------\n",
    "    from source.assessment_contributions import compute_forecasters_contributions\n",
    "    from source.plots.display_contributions import permutation_pivot_data\n",
    "    from matplotlib import pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    if ens_params['model_type'] == 'LR' and ens_params['var_model_type'] == 'LR':\n",
    "        logger.info(' ')\n",
    "        logger.opt(colors = True).info('<blue> -----------------> Forecasters LASSO coefficients contributions computed </blue>')\n",
    "        import pickle\n",
    "        from source.simulation.helpers_simulation import compute_coefficients\n",
    "        from source.plots.display_contributions import lasso_coefs_pivot_data\n",
    "        with open('/Users/gio/Desktop/Elia-RES-Forecasting/info_model/b1r1_previous_day.pickle', 'rb') as handle:\n",
    "            previous_day = pickle.load(handle)\n",
    "        iter_coefficients_contributions = compute_coefficients(ens_params, previous_day, p_values=True)\n",
    "        df_pivot = lasso_coefs_pivot_data(sim_params, iter_coefficients_contributions)\n",
    "        title = 'Lasso Coefficients'\n",
    "        plot_stacked_bar_chart(df_pivot, title)\n",
    "        plot_heatmap(df_pivot.T, title)\n",
    "\n",
    "    logger.info(' -----------------> Forecasters  Permutation Contributions')\n",
    "    logger.info(' ')\n",
    "    ens_params['contribution_method'] = 'permutation'\n",
    "    contr_mthd = ens_params['contribution_method']\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters {contr_mthd} contributions computed </blue>')\n",
    "    y_test = df_test['measured'].values\n",
    "    iter_permutation_contributions = compute_forecasters_contributions(buyer_resource_name, ens_params, y_test, forecast_range)\n",
    "    avg_permutation_contributions = update_dict_weights(avg_permutation_contributions, iter_permutation_contributions, iteration=i)\n",
    "\n",
    "    df_pivot = permutation_pivot_data(sim_params, iter_permutation_contributions)\n",
    "    \n",
    "    # Plot the stacked bar chart\n",
    "    df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "    plt.xlabel('Key and Quantile')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Stacked Bar Chart of Daily Permutation Contributions')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.heatmap(df_pivot.T, annot=True, cmap='viridis')\n",
    "    plt.xlabel('Series')\n",
    "    plt.ylabel('Key and Quantile')\n",
    "    plt.title('Heatmap of Daily Permutation Contributions')\n",
    "    plt.show()\n",
    "\n",
    "    ens_params['contribution_method'] = 'shapley'\n",
    "    contr_mthd = ens_params['contribution_method']\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'<blue> -----------------> Forecasters {contr_mthd} contributions computed </blue>')\n",
    "    y_test = df_test['measured'].values\n",
    "    iter_shapley_contributions = compute_forecasters_contributions(buyer_resource_name, ens_params, y_test, forecast_range)\n",
    "    avg_shapley_contributions = update_dict_weights(avg_shapley_contributions, iter_shapley_contributions, iteration=i)\n",
    "\n",
    "    logger.info(' ')\n",
    "    logger.opt(colors = True).info(f'{iter_shapley_contributions}')\n",
    "    df_pivot = permutation_pivot_data(sim_params, iter_shapley_contributions)\n",
    "\n",
    "    # Plot the stacked bar chart\n",
    "    df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "    plt.xlabel('Key and Quantile')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title('Stacked Bar Chart of Daily Shapley Contributions')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    sns.heatmap(df_pivot.T, annot=True, cmap='viridis')\n",
    "    plt.xlabel('Series')\n",
    "    plt.ylabel('Key and Quantile')\n",
    "    plt.title('Heatmap of Daily Shapley Contributions')\n",
    "    plt.show()\n",
    "\n",
    "    # clear_output(wait=True)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_quantile_columns(df, quantile):\n",
    "    \"\"\"Extract columns containing the specified quantile.\"\"\"\n",
    "    columns = [name for name in df.columns if quantile in name]\n",
    "    if columns:\n",
    "        return df[columns]\n",
    "    else:\n",
    "        print(f\"No columns found for {quantile}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "extract_quantile_columns(df_market, 'q10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market\n",
    "# drop columns containing either q10 or q90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = permutation_pivot_data(sim_params, avg_permutation_contributions)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Chart of Average Permutation Contributions')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot.T, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title('Heatmap of Average Permutation Contributions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot = permutation_pivot_data(sim_params, avg_shapley_contributions)\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "df_pivot.plot(kind='bar', stacked=True, figsize=(15, 6))\n",
    "plt.xlabel('Key and Quantile')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Stacked Bar Chart of Average Shapley Contributions')\n",
    "plt.show()\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.heatmap(df_pivot.T, annot=True, cmap='viridis')\n",
    "plt.xlabel('Series')\n",
    "plt.ylabel('Key and Quantile')\n",
    "plt.title('Heatmap of Average Shapley Contributions')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
