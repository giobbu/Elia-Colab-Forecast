{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import pinball loss from sklearn\n",
    "from sklearn.metrics import mean_pinball_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "from config.simulation_setting import Simulation\n",
    "\n",
    "sim_params = Simulation.testing_period  # Simulation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv file\n",
    "if sim_params['most_recent']:\n",
    "    df = pd.read_csv('wp_forecasters_comparison_results.csv')\n",
    "else:\n",
    "    df = pd.read_csv('wp_forecasters_comparison_results_no_mostrecent.csv')\n",
    "print(df.shape)\n",
    "\n",
    "# min and max values for datetime\n",
    "min_date = df['datetime'].min()\n",
    "max_date = df['datetime'].max()\n",
    "print('Min date:', min_date)\n",
    "print('Max date:', max_date)\n",
    "\n",
    "df_clean = df.rename(columns={'10_predictions': 'q10_QRA', '50_predictions': 'q50_QRA', '90_predictions': 'q90_QRA'})\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the two dataframes\n",
    "df = df_clean.copy() #.set_index('datetime').join(df_stack_clean.set_index('datetime')[['q10_stack', 'q50_stack', 'q90_stack']], on='datetime').reset_index()\n",
    "# add date column\n",
    "df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
    "# add month-year column\n",
    "df['month_year'] = pd.to_datetime(df['datetime']).dt.to_period('M')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rmse\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "# calculate mean pinball loss with sklearn\n",
    "def mean_pinball_loss_sklearn(targets, predictions, tau):\n",
    "    return mean_pinball_loss(targets, predictions, alpha=tau)\n",
    "\n",
    "\n",
    "def WIS(y_true, lower, upper, alpha):\n",
    "    \" Winkler Interval Score (WIS) for a single row\"\n",
    "\n",
    "    assert np.isnan(y_true) == False, \"y_true contains NaN value(s)\"\n",
    "    assert np.isinf(y_true) == False, \"y_true contains inf values(s)\"\n",
    "    assert np.isnan(lower)  == False, \"lower interval value contains NaN value(s)\"\n",
    "    assert np.isinf(lower)  == False, \"lower interval value contains inf values(s)\"\n",
    "    assert np.isnan(upper)  == False, \"upper interval value contains NaN value(s)\"\n",
    "    assert np.isinf(upper)  == False, \"upper interval value contains inf values(s)\"\n",
    "    assert alpha > 0 and alpha <= 1,  f\"alpha should be (0,1]. Found: {alpha}\"\n",
    "\n",
    "    # WIS for one single row\n",
    "    score = np.abs(upper-lower)\n",
    "    if y_true < np.minimum(upper,lower):\n",
    "        score += ((2/alpha) * (np.minimum(upper, lower) - y_true))\n",
    "    if y_true > np.maximum(upper,lower):\n",
    "        score += ((2/alpha) * (y_true - np.maximum(upper,lower)))\n",
    "    return score\n",
    "\n",
    "# vectorize the function\n",
    "v_WIS = np.vectorize(WIS)\n",
    "\n",
    "def mean_winkler_score(y_true, lower, upper, alpha):\n",
    "    \" Mean Winkler Interval Score (MWIS) for a pandas Series or 1D array\"\n",
    "    assert y_true.ndim == 1, \"y_true: pandas Series or 1D array expected\"\n",
    "    assert lower.ndim  == 1, \"lower: pandas Series or 1D array expected\"\n",
    "    assert upper.ndim  == 1, \"upper: pandas Series or 1D array expected\"\n",
    "    assert isinstance(alpha, float) == True, \"alpha: float expected\"\n",
    "    WIS_scores = v_WIS(y_true,lower,upper,alpha)\n",
    "    MWIS      = np.mean(WIS_scores)\n",
    "    MWIS      = float(MWIS)\n",
    "    return MWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse_testing_days(df, predictions, targets = 'measured', date_column = 'date'):\n",
    "    \" Compute RMSE for each day in the dataframe \"\n",
    "    rmse_list = []\n",
    "    for date in df[date_column].unique():\n",
    "        date_df = df[df[date_column] == date]\n",
    "        rmse_list.append(rmse(date_df[targets], date_df[predictions]))\n",
    "    return rmse_list\n",
    "\n",
    "def compute_pinball_loss_testing_days(df, tau, predictions, targets = 'measured', date_column = 'date'):\n",
    "    \" Compute Pinball Loss for each day in the dataframe \"\n",
    "    pinball_loss_list = []\n",
    "    for date in df[date_column].unique():\n",
    "        date_df = df[df[date_column] == date]\n",
    "        pinball_loss_list.append(mean_pinball_loss_sklearn(date_df[targets], date_df[predictions], tau))\n",
    "    return pinball_loss_list\n",
    "\n",
    "def compute_winkler_score_testing_days(df, alpha, predictions_q10, predictions_q90, targets = 'measured', date_column = 'date'):\n",
    "    \" Compute Winkler Score for each day in the dataframe \"\n",
    "    winkler_score_list = []\n",
    "    for date in df[date_column].unique():\n",
    "        date_df = df[df[date_column] == date]\n",
    "        winkler_score_list.append(mean_winkler_score(date_df[targets], date_df[predictions_q10], date_df[predictions_q90], alpha))\n",
    "    return winkler_score_list\n",
    "\n",
    "def compute_coverage_testing_days(df, predictions_q10, predictions_q90, targets = 'measured', date_column = 'date'):\n",
    "    \" Compute coverage for each day in the dataframe \"\n",
    "    coverage_list = []\n",
    "    for date in df[date_column].unique():\n",
    "        date_df = df[df[date_column] == date]\n",
    "        coverage_list.append(np.mean((date_df[predictions_q10] <= date_df[targets]) & (date_df[predictions_q90] >= date_df[targets])))\n",
    "    return coverage_list\n",
    "\n",
    "def compute_sharpness_testing_days(df, predictions_q10, predictions_q90, date_column = 'date'):\n",
    "    \" Compute sharpness for each day in the dataframe \"\n",
    "    sharpness_list = []\n",
    "    for date in df[date_column].unique():\n",
    "        date_df = df[df[date_column] == date]\n",
    "        sharpness_list.append(np.mean(date_df[predictions_q90] - date_df[predictions_q10]))\n",
    "    return sharpness_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot rmse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_ts_loss(df, list_loss_1, list_loss_2, model_name_1, model_name_2, date_column = 'date'):\n",
    "    \"\"\" Plot timeseries loss\"\"\"\n",
    "    assert len(list_loss_1) == len(list_loss_2), \"The two lists must have the same length\"\n",
    "    assert len(list_loss_1) == len(df[date_column].unique()), \"The length of the list must be equal to the number of unique dates in the dataframe\"\n",
    "    assert len(list_loss_2) == len(df[date_column].unique()), \"The length of the list must be equal to the number of unique dates in the dataframe\"\n",
    "    # plot ensemble\n",
    "    plt.figure(figsize=(25, 7))\n",
    "    # x-axis date, y-axis rmse\n",
    "    if date_column == 'date':\n",
    "        timestamps = df[date_column].unique()\n",
    "    else:\n",
    "        timestamps = df[date_column].unique().to_timestamp()\n",
    "    plt.plot(timestamps, list_loss_1, label=model_name_1)\n",
    "    plt.plot(timestamps, list_loss_2, label=model_name_2, alpha=0.5, linestyle='dashed')\n",
    "    # fill between the two models\n",
    "    plt.fill_between(timestamps, list_loss_1, list_loss_2, color='red', alpha=0.3)\n",
    "    plt.title('RMSE by date')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.xlabel('Date')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Date-based analysis ----- \n",
    "\n",
    "# results for the ensemble model QRA\n",
    "# calculate rmse\n",
    "rmse_list_QRA = compute_rmse_testing_days(df, predictions='q50_QRA')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_QRA = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_QRA')\n",
    "pinball_loss_09_list_QRA = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_QRA')\n",
    "# calculate Winkler Interval Score\n",
    "winkler_score_list_QRA = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='q10_QRA', predictions_q90='q90_QRA')\n",
    "# calculate coverage\n",
    "coverage_list_QRA = compute_coverage_testing_days(df, predictions_q10='q10_QRA', predictions_q90='q90_QRA')\n",
    "# calculate sharpness\n",
    "sharpness_list_QRA = compute_sharpness_testing_days(df, predictions_q10='q10_QRA', predictions_q90='q90_QRA')\n",
    "\n",
    "# results for the day-ahead model\n",
    "# calculate rmse\n",
    "rmse_list_dayahead = compute_rmse_testing_days(df, predictions='dayaheadforecast')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_dayahead = compute_pinball_loss_testing_days(df, tau=0.1, predictions='dayaheadconfidence10')\n",
    "pinball_loss_09_list_dayahead = compute_pinball_loss_testing_days(df, tau=0.9, predictions='dayaheadconfidence90')\n",
    "# calculate winkler score\n",
    "winkler_score_list_dayahead = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='dayaheadconfidence10', predictions_q90='dayaheadconfidence90')\n",
    "# calculate coverage\n",
    "coverage_list_dayahead = compute_coverage_testing_days(df, predictions_q10='dayaheadconfidence10', predictions_q90='dayaheadconfidence90')\n",
    "# calculate sharpness\n",
    "sharpness_list_dayahead = compute_sharpness_testing_days(df, predictions_q10='dayaheadconfidence10', predictions_q90='dayaheadconfidence90')\n",
    "\n",
    "# results for dayahead11hforecast model\n",
    "# calculate rmse\n",
    "rmse_list_dayahead11h = compute_rmse_testing_days(df, predictions='dayahead11hforecast')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_dayahead11h = compute_pinball_loss_testing_days(df, tau=0.1, predictions='dayahead11hconfidence10')\n",
    "pinball_loss_09_list_dayahead11h = compute_pinball_loss_testing_days(df, tau=0.9, predictions='dayahead11hconfidence90')\n",
    "# calculate winkler score\n",
    "winkler_score_list_dayahead11h = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='dayahead11hconfidence10', predictions_q90='dayahead11hconfidence90')\n",
    "# calculate coverage\n",
    "coverage_list_dayahead11h = compute_coverage_testing_days(df, predictions_q10='dayahead11hconfidence10', predictions_q90='dayahead11hconfidence90')\n",
    "# calculate sharpness\n",
    "sharpness_list_dayahead11h = compute_sharpness_testing_days(df, predictions_q10='dayahead11hconfidence10', predictions_q90='dayahead11hconfidence90')\n",
    "\n",
    "# results for weekaheadforecast model\n",
    "# calculate rmse\n",
    "rmse_list_weekahead = compute_rmse_testing_days(df, predictions='weekaheadforecast')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weekahead = compute_pinball_loss_testing_days(df, tau=0.1, predictions='weekaheadconfidence10')\n",
    "pinball_loss_09_list_weekahead = compute_pinball_loss_testing_days(df, tau=0.9, predictions='weekaheadconfidence90')\n",
    "# calculate winkler score\n",
    "winkler_score_list_weekahead = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='weekaheadconfidence10', predictions_q90='weekaheadconfidence90')\n",
    "# calculate coverage\n",
    "coverage_list_weekahead = compute_coverage_testing_days(df, predictions_q10='weekaheadconfidence10', predictions_q90='weekaheadconfidence90')\n",
    "# calculate sharpness\n",
    "sharpness_list_weekahead = compute_sharpness_testing_days(df, predictions_q10='weekaheadconfidence10', predictions_q90='weekaheadconfidence90')\n",
    "\n",
    "# results for 'q10_best_model', 'q50_best_model', 'q90_best_model' model\n",
    "# calculate rmse\n",
    "rmse_list_best = compute_rmse_testing_days(df, predictions='q50_best_model')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_best = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_best_model')\n",
    "pinball_loss_09_list_best = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_best_model')\n",
    "# calculate winkler score\n",
    "winkler_score_list_best = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='q10_best_model', predictions_q90='q90_best_model')\n",
    "# calculate coverage\n",
    "coverage_list_best = compute_coverage_testing_days(df, predictions_q10='q10_best_model', predictions_q90='q90_best_model')\n",
    "# calculate sharpness\n",
    "sharpness_list_best = compute_sharpness_testing_days(df, predictions_q10='q10_best_model', predictions_q90='q90_best_model')\n",
    "\n",
    "# results for 'q10_weight_avg', 'q50_weight_avg', 'q90_weight_avg' model\n",
    "# calculate rmse\n",
    "rmse_list_weight_avg = compute_rmse_testing_days(df, predictions='q50_weight_avg')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weight_avg = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_weight_avg')\n",
    "pinball_loss_09_list_weight_avg = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_weight_avg')\n",
    "# calculate winkler score\n",
    "winkler_score_list_weight_avg = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='q10_weight_avg', predictions_q90='q90_weight_avg')\n",
    "# calculate coverage\n",
    "coverage_list_weight_avg = compute_coverage_testing_days(df, predictions_q10='q10_weight_avg', predictions_q90='q90_weight_avg')\n",
    "# calculate sharpness\n",
    "sharpness_list_weight_avg = compute_sharpness_testing_days(df, predictions_q10='q10_weight_avg', predictions_q90='q90_weight_avg')\n",
    "\n",
    "# results for 'q10_weight_avg_soft', 'q50_weight_avg_soft', 'q90_weight_avg_soft' model\n",
    "# calculate rmse\n",
    "rmse_list_weight_avg_soft = compute_rmse_testing_days(df, predictions='q50_weight_avg_soft')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weight_avg_soft = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_weight_avg_soft')\n",
    "pinball_loss_09_list_weight_avg_soft = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_weight_avg_soft')\n",
    "# calculate winkler score\n",
    "winkler_score_list_weight_avg_soft = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='q10_weight_avg_soft', predictions_q90='q90_weight_avg_soft')\n",
    "# calculate coverage\n",
    "coverage_list_weight_avg_soft = compute_coverage_testing_days(df, predictions_q10='q10_weight_avg_soft', predictions_q90='q90_weight_avg_soft')\n",
    "# calculate sharpness\n",
    "sharpness_list_weight_avg_soft = compute_sharpness_testing_days(df, predictions_q10='q10_weight_avg_soft', predictions_q90='q90_weight_avg_soft')\n",
    "\n",
    "# results for 'q10_equal_weights', 'q50_equal_weights', 'q90_equal_weights' model\n",
    "# calculate rmse\n",
    "rmse_list_equal_weights = compute_rmse_testing_days(df, predictions='q50_equal_weights')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_equal_weights = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_equal_weights')\n",
    "pinball_loss_09_list_equal_weights = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_equal_weights')\n",
    "# calculate winkler score\n",
    "winkler_score_list_equal_weights = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='q10_equal_weights', predictions_q90='q90_equal_weights')\n",
    "# calculate coverage\n",
    "coverage_list_equal_weights = compute_coverage_testing_days(df, predictions_q10='q10_equal_weights', predictions_q90='q90_equal_weights')\n",
    "# calculate sharpness\n",
    "sharpness_list_equal_weights = compute_sharpness_testing_days(df, predictions_q10='q10_equal_weights', predictions_q90='q90_equal_weights')\n",
    "\n",
    "if sim_params['most_recent']:\n",
    "    # results for 'mostrecentconfidence10', 'mostrecentconfidence90', 'mostrecentforecast' model\n",
    "    # calculate rmse\n",
    "    rmse_list_most_recent = compute_rmse_testing_days(df, predictions='mostrecentforecast')\n",
    "    # calculate pinball loss\n",
    "    pinball_loss_01_list_most_recent = compute_pinball_loss_testing_days(df, tau=0.1, predictions='mostrecentconfidence10')\n",
    "    pinball_loss_09_list_most_recent = compute_pinball_loss_testing_days(df, tau=0.9, predictions='mostrecentconfidence90')\n",
    "    # calculate winkler score\n",
    "    winkler_score_list_most_recent = compute_winkler_score_testing_days(df, alpha=0.1, predictions_q10='mostrecentconfidence10', predictions_q90='mostrecentconfidence90')\n",
    "    # calculate coverage\n",
    "    coverage_list_most_recent = compute_coverage_testing_days(df, predictions_q10='mostrecentconfidence10', predictions_q90='mostrecentconfidence90')\n",
    "    # calculate sharpness\n",
    "    sharpness_list_most_recent = compute_sharpness_testing_days(df, predictions_q10='mostrecentconfidence10', predictions_q90='mostrecentconfidence90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Motnhly-Year-based analysis ----- \n",
    "\n",
    "# results for the ensemble model QRA\n",
    "# calculate rmse\n",
    "rmse_list_QRA_month_year = compute_rmse_testing_days(df, predictions='q50_QRA', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_QRA_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_QRA', date_column='month_year')\n",
    "pinball_loss_09_list_QRA_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_QRA', date_column='month_year')\n",
    "\n",
    "# results for the day-ahead model\n",
    "# calculate rmse\n",
    "rmse_list_dayahead_month_year = compute_rmse_testing_days(df, predictions='dayaheadforecast', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_dayahead_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='dayaheadconfidence10', date_column='month_year')\n",
    "pinball_loss_09_list_dayahead_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='dayaheadconfidence90', date_column='month_year')\n",
    "\n",
    "# results for dayahead11hforecast model\n",
    "# calculate rmse\n",
    "rmse_list_dayahead11h_month_year = compute_rmse_testing_days(df, predictions='dayahead11hforecast', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_dayahead11h_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='dayahead11hconfidence10', date_column='month_year')\n",
    "pinball_loss_09_list_dayahead11h_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='dayahead11hconfidence90', date_column='month_year')\n",
    "\n",
    "# results for weekaheadforecast model\n",
    "# calculate rmse\n",
    "rmse_list_weekahead_month_year = compute_rmse_testing_days(df, predictions='weekaheadforecast', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weekahead_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='weekaheadconfidence10', date_column='month_year')\n",
    "pinball_loss_09_list_weekahead_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='weekaheadconfidence90', date_column='month_year')\n",
    "\n",
    "# results for 'q10_best_model', 'q50_best_model', 'q90_best_model' model\n",
    "# calculate rmse\n",
    "rmse_list_best_month_year = compute_rmse_testing_days(df, predictions='q50_best_model', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_best_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_best_model', date_column='month_year')\n",
    "pinball_loss_09_list_best_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_best_model', date_column='month_year')\n",
    "\n",
    "# results for 'q10_weight_avg', 'q50_weight_avg', 'q90_weight_avg' model\n",
    "# calculate rmse\n",
    "rmse_list_weight_avg_month_year = compute_rmse_testing_days(df, predictions='q50_weight_avg', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weight_avg_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_weight_avg', date_column='month_year')\n",
    "pinball_loss_09_list_weight_avg_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_weight_avg', date_column='month_year')\n",
    "\n",
    "# results for 'q10_weight_avg_soft', 'q50_weight_avg_soft', 'q90_weight_avg_soft' model\n",
    "# calculate rmse\n",
    "rmse_list_weight_avg_soft_month_year = compute_rmse_testing_days(df, predictions='q50_weight_avg_soft', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_weight_avg_soft_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_weight_avg_soft', date_column='month_year')\n",
    "pinball_loss_09_list_weight_avg_soft_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_weight_avg_soft', date_column='month_year')\n",
    "\n",
    "# results for 'q10_equal_weights', 'q50_equal_weights', 'q90_equal_weights' model\n",
    "# calculate rmse\n",
    "rmse_list_equal_weights_month_year = compute_rmse_testing_days(df, predictions='q50_equal_weights', date_column='month_year')\n",
    "# calculate pinball loss\n",
    "pinball_loss_01_list_equal_weights_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='q10_equal_weights', date_column='month_year')\n",
    "pinball_loss_09_list_equal_weights_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='q90_equal_weights', date_column='month_year')\n",
    "\n",
    "if sim_params['most_recent']:\n",
    "    # results for 'mostrecentconfidence10', 'mostrecentconfidence90', 'mostrecentforecast' model\n",
    "    # calculate rmse\n",
    "    rmse_list_most_recent_month_year = compute_rmse_testing_days(df, predictions='mostrecentforecast', date_column='month_year')\n",
    "    # calculate pinball loss\n",
    "    pinball_loss_01_list_most_recent_month_year = compute_pinball_loss_testing_days(df, tau=0.1, predictions='mostrecentconfidence10', date_column='month_year')\n",
    "    pinball_loss_09_list_most_recent_month_year = compute_pinball_loss_testing_days(df, tau=0.9, predictions='mostrecentconfidence90', date_column='month_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def barplot(sim_params, title, loss_list_QRA, loss_list_dayahead, loss_list_dayahead11h, loss_list_weekahead, loss_list_best, loss_list_weight_avg, loss_list_weight_avg_soft, loss_list_equal_weights, loss_list_most_recent=None):\n",
    "        \" Plot barplot of mean rmse with std\"\n",
    "        # compute mean rmse and std\n",
    "        mean_loss_QRA, err_loss_QRA = np.mean(loss_list_QRA), np.std(loss_list_QRA)/np.sqrt(len(loss_list_QRA))\n",
    "        mean_loss_dayahead, err_loss_dayahead = np.mean(loss_list_dayahead), np.std(loss_list_dayahead)/np.sqrt(len(loss_list_dayahead))\n",
    "        mean_loss_dayahead11h, err_loss_dayahead11h = np.mean(loss_list_dayahead11h), np.std(loss_list_dayahead11h)/np.sqrt(len(loss_list_dayahead11h))\n",
    "        mean_loss_weekahead, err_loss_weekahead = np.mean(loss_list_weekahead), np.std(loss_list_weekahead)/np.sqrt(len(loss_list_weekahead))\n",
    "        mean_loss_best_model, err_loss_best_model = np.mean(loss_list_best), np.std(loss_list_best)/np.sqrt(len(loss_list_best))\n",
    "        mean_loss_weighted_avg, err_loss_weighted_avg = np.mean(loss_list_weight_avg), np.std(loss_list_weight_avg)/np.sqrt(len(loss_list_weight_avg))\n",
    "        mean_loss_weighted_avg_soft, err_loss_weighted_avg_soft = np.mean(loss_list_weight_avg_soft), np.std(loss_list_weight_avg_soft)/np.sqrt(len(loss_list_weight_avg_soft))\n",
    "        mean_loss_equal_weights, err_loss_equal_weights = np.mean(loss_list_equal_weights), np.std(loss_list_equal_weights)/np.sqrt(len(loss_list_equal_weights))\n",
    "        if sim_params['most_recent']:\n",
    "                mean_loss_most_recent, err_loss_most_recent = np.mean(loss_list_most_recent), np.std(loss_list_most_recent)/np.sqrt(len(loss_list_most_recent))\n",
    "\n",
    "        # percentage of improvement of QRA over the other models\n",
    "        improvement_dayahead = (mean_loss_dayahead - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_dayahead11h = (mean_loss_dayahead11h - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_weekahead = (mean_loss_weekahead - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_best_model = (mean_loss_best_model - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_weighted_avg = (mean_loss_weighted_avg - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_weighted_avg_soft = (mean_loss_weighted_avg_soft - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        improvement_equal_weights = (mean_loss_equal_weights - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "        if sim_params['most_recent']:\n",
    "                improvement_most_recent = (mean_loss_most_recent - mean_loss_QRA) / mean_loss_QRA * 100\n",
    "\n",
    "        # plot seaborn barplot of mean rmse with std\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        data = {'Forecaster': ['QRA', 'Best Selected', 'Equal Weights', 'Weighted Avg', 'Weighted Avg Soft', 'Day-ahead', 'Day-ahead 11h', 'Week-ahead'],\n",
    "                'Mean': [mean_loss_QRA, mean_loss_best_model, mean_loss_equal_weights, mean_loss_weighted_avg, mean_loss_weighted_avg_soft, mean_loss_dayahead, mean_loss_dayahead11h, mean_loss_weekahead],\n",
    "                'Std Error': [err_loss_QRA, err_loss_best_model, err_loss_equal_weights, err_loss_weighted_avg, err_loss_weighted_avg_soft, err_loss_dayahead, err_loss_dayahead11h, err_loss_weekahead],\n",
    "                'Improvement (%)': [0, improvement_best_model, improvement_equal_weights, improvement_weighted_avg, improvement_weighted_avg_soft, improvement_dayahead, improvement_dayahead11h, improvement_weekahead]}\n",
    "        if sim_params['most_recent']:\n",
    "                data['Forecaster'].append('Most Recent')\n",
    "                data['Mean'].append(mean_loss_most_recent)\n",
    "                data['Std Error'].append(err_loss_most_recent)\n",
    "                data['Improvement (%)'].append(improvement_most_recent)\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        if 'Coverage' in title:\n",
    "                sns.barplot(x='Mean', y='Forecaster', data=df, alpha=0.5, color='green')\n",
    "                # add error bars\n",
    "                plt.errorbar(df['Mean'], df['Forecaster'], xerr=df['Std Error'], fmt='o')\n",
    "                for i in range(len(df)):\n",
    "                        plt.text(df['Mean'][i] + df['Std Error'][i], i, f\" {df['Mean'][i]:.2f}\", va='center', ha='left', rotation=0, fontsize=10)\n",
    "                # plot vertical dashed red line for the maximum mean value\n",
    "                plt.axvline(0.8, color='red', linestyle='dashed')\n",
    "                plt.title(title)\n",
    "        elif 'Sharpness' in title:\n",
    "                sns.barplot(x='Mean', y='Forecaster', data=df, alpha=0.5, color='purple')\n",
    "                # add error bars\n",
    "                plt.errorbar(df['Mean'], df['Forecaster'], xerr=df['Std Error'], fmt='o')\n",
    "                for i in range(len(df)):\n",
    "                        plt.text(df['Mean'][i] + df['Std Error'][i], i, f\" {df['Mean'][i]:.2f}\", va='center', ha='left', rotation=0, fontsize=10)\n",
    "                # plot vertical dashed red line for the maximum mean value\n",
    "                plt.axvline(df['Mean'].min(), color='red', linestyle='dashed')\n",
    "                plt.title(title)\n",
    "        elif 'Coverage' or 'Sharpness' not in title:\n",
    "                sns.barplot(x='Mean', y='Forecaster', data=df, alpha=0.5, color='blue')\n",
    "                # add error bars\n",
    "                plt.errorbar(df['Mean'], df['Forecaster'], xerr=df['Std Error'], fmt='o')\n",
    "                # add percentage of improvement where text rotation is horizontal and smaller\n",
    "                for i in range(len(df)):\n",
    "                        plt.text(df['Mean'][i] + df['Std Error'][i], i, f\" {df['Improvement (%)'][i]:.2f}%\", va='center', ha='left', rotation=0, fontsize=10)\n",
    "                # plot vertical dashed red line for the minimum mean value\n",
    "                plt.axvline(df['Mean'].min(), color='red', linestyle='dashed')\n",
    "                plt.title(f'{title} (QRA % improvement over other models)')\n",
    "        else:\n",
    "                # raise an error if the title is not correct\n",
    "                raise ValueError('The title must contain either RMSE, Pinball Loss, Coverage or Sharpness')\n",
    "                \n",
    "        \n",
    "        # save figure\n",
    "        # split the title by space and join by underscore\n",
    "        title = '_'.join(title.split())\n",
    "        plt.savefig(f'{title}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse\n",
    "list_QRA_over_dayahead = [1 if rmse_QRA <rmse_dayahead else 0 for rmse_QRA in rmse_list_QRA for rmse_dayahead in rmse_list_dayahead]\n",
    "list_QRA_over_dayahead11h = [1 if rmse_QRA <rmse_dayahead11h else 0 for rmse_QRA in rmse_list_QRA for rmse_dayahead11h in rmse_list_dayahead11h]\n",
    "list_QRA_over_weekahead = [1 if rmse_QRA <rmse_weekahead else 0 for rmse_QRA in rmse_list_QRA for rmse_weekahead in rmse_list_weekahead]\n",
    "list_QRA_over_best = [1 if rmse_QRA <rmse_best else 0 for rmse_QRA in rmse_list_QRA for rmse_best in rmse_list_best]\n",
    "list_QRA_over_equal_weights = [1 if rmse_QRA <rmse_equal_weights else 0 for rmse_QRA in rmse_list_QRA for rmse_equal_weights in rmse_list_equal_weights]\n",
    "list_QRA_over_weight_avg = [1 if rmse_QRA <rmse_weight_avg else 0 for rmse_QRA in rmse_list_QRA for rmse_weight_avg in rmse_list_weight_avg]\n",
    "list_QRA_over_weight_avg_soft = [1 if rmse_QRA <rmse_weight_avg_soft else 0 for rmse_QRA in rmse_list_QRA for rmse_weight_avg_soft in rmse_list_weight_avg_soft]\n",
    "if sim_params['most_recent']:\n",
    "    list_QRA_over_most_recent = [1 if rmse_QRA <rmse_most_recent else 0 for rmse_QRA in rmse_list_QRA for rmse_most_recent in rmse_list_most_recent]\n",
    "\n",
    "# create dataframe\n",
    "start_index = 0\n",
    "end_index = -1\n",
    "df_rmse = pd.DataFrame({'QRA': rmse_list_QRA, 'Day-ahead': rmse_list_dayahead, 'Day-ahead 11h': rmse_list_dayahead11h, 'Week-ahead': rmse_list_weekahead, 'Best Selected': rmse_list_best, 'Equal Weights': rmse_list_equal_weights, 'Weighted Avg': rmse_list_weight_avg, 'Weighted Avg Soft': rmse_list_weight_avg_soft})\n",
    "if sim_params['most_recent']:\n",
    "    df_rmse['Most Recent'] = rmse_list_most_recent\n",
    "df_rmse = df_rmse.iloc[start_index:end_index]\n",
    "\n",
    "# frequency QRA has the lowest RMSE over the other models df_rmse\n",
    "n_best = df_rmse.apply(lambda x: x.idxmin(), axis=1).value_counts()/df_rmse.shape[0]\n",
    "print('Frequency models with the lowest RMSE:')\n",
    "print(n_best )\n",
    "print(' ')\n",
    "# frequency QRA has the highest RMSE over the other models df_rmse\n",
    "n_worst = df_rmse.apply(lambda x: x.idxmax(), axis=1).value_counts()/df_rmse.shape[0]\n",
    "print('Frequency models with the highest RMSE:')\n",
    "print(n_worst)\n",
    "print(' ')\n",
    "\n",
    "# create dataframe where lowest RMSE is 1 and highest RMSE is -1 and others are 0 per date\n",
    "df_binary = pd.DataFrame(index=df_rmse.index, columns=df_rmse.columns)\n",
    "for date in df_rmse.index:\n",
    "    df_binary.loc[date] = np.where(df_rmse.loc[date] == df_rmse.loc[date].min(), float(1.0), \n",
    "                        np.where(df_rmse.loc[date] == df_rmse.loc[date].max(), float(-1.0), \n",
    "                        float(0.0)))\n",
    "\n",
    "# plot heatmap for df_binary\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df_binary.T.astype(float), cmap='RdYlGn', ax=ax)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('forecasters')\n",
    "plt.title('Best RMSE per date')\n",
    "plt.show()\n",
    "\n",
    "# # create a dataframe where the value of RMSE from ensemble_Q50 and from week-ahead remain the same while the rest are 0\n",
    "# df_binary_weekahead = pd.DataFrame(index=df_rmse.index, columns=df_rmse.columns)\n",
    "# for date in df_rmse.index:\n",
    "#     df_binary_weekahead.loc[date] = np.where(df_rmse.loc[date] == df_rmse.loc[date]['QRA'], df_rmse.loc[date]['QRA'], \n",
    "#                         #np.where(df_rmse.loc[date] == df_rmse.loc[date]['Week-ahead'], df_rmse.loc[date]['Week-ahead'], \n",
    "#                         float(0.0))\n",
    "# df_binary_weekahead = df_binary_weekahead.astype(float)\n",
    "\n",
    "# # plot heatmap for df_binary_weekahead\n",
    "# fig, ax = plt.subplots(figsize=(20, 10))\n",
    "# sns.heatmap(df_binary_weekahead.T, cmap='viridis', ax=ax)\n",
    "# plt.xlabel('date')\n",
    "# plt.ylabel('forecasters')\n",
    "# plt.title('Best RMSE per date')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinball loss 0.1\n",
    "list_QRA_over_dayahead_01 = [1 if pinball_loss_QRA <pinball_loss_dayahead else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_dayahead in pinball_loss_01_list_dayahead]\n",
    "list_QRA_over_dayahead11h_01 = [1 if pinball_loss_QRA <pinball_loss_dayahead11h else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_dayahead11h in pinball_loss_01_list_dayahead11h]\n",
    "list_QRA_over_weekahead_01 = [1 if pinball_loss_QRA <pinball_loss_weekahead else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_weekahead in pinball_loss_01_list_weekahead]\n",
    "list_QRA_over_best_01 = [1 if pinball_loss_QRA <pinball_loss_best else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_best in pinball_loss_01_list_best]\n",
    "list_QRA_over_equal_weights_01 = [1 if pinball_loss_QRA <pinball_loss_equal_weights else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_equal_weights in pinball_loss_01_list_equal_weights]\n",
    "list_QRA_over_weight_avg_01 = [1 if pinball_loss_QRA <pinball_loss_weight_avg else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_weight_avg in pinball_loss_01_list_weight_avg]\n",
    "list_QRA_over_weight_avg_soft_01 = [1 if pinball_loss_QRA <pinball_loss_weight_avg_soft else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_weight_avg_soft in pinball_loss_01_list_weight_avg_soft]\n",
    "if sim_params['most_recent']:\n",
    "    list_QRA_over_most_recent_01 = [1 if pinball_loss_QRA <pinball_loss_most_recent else 0 for pinball_loss_QRA in pinball_loss_01_list_QRA for pinball_loss_most_recent in pinball_loss_01_list_most_recent]\n",
    "\n",
    "# create dataframe\n",
    "start_index = 0\n",
    "end_index = -1\n",
    "df_pinball_q10 = pd.DataFrame({'QRA': pinball_loss_01_list_QRA, 'Day-ahead': pinball_loss_01_list_dayahead, 'Day-ahead 11h': pinball_loss_01_list_dayahead11h, 'Week-ahead': pinball_loss_01_list_weekahead, 'Best Selected': pinball_loss_01_list_best, 'Equal Weights': pinball_loss_01_list_equal_weights, 'Weighted Avg': pinball_loss_01_list_weight_avg, 'Weighted Avg Soft': pinball_loss_01_list_weight_avg_soft})\n",
    "if sim_params['most_recent']:\n",
    "    df_pinball_q10['Most Recent'] = pinball_loss_01_list_most_recent\n",
    "df_pinball_q10 = df_pinball_q10.iloc[start_index:end_index]\n",
    "\n",
    "# frequency QRA has the lowest Pinball over the other models df_pinball_q10\n",
    "n_best = df_pinball_q10.apply(lambda x: x.idxmin(), axis=1).value_counts()/df_pinball_q10.shape[0]\n",
    "print('Frequency models with the lowest Pinball Loss 0.1:')\n",
    "print(n_best)\n",
    "print('sum', n_best.sum())\n",
    "print(' ')\n",
    "# frequency QRA has the highest Pinball over the other models df_pinball_q10\n",
    "n_worst = df_pinball_q10.apply(lambda x: x.idxmax(), axis=1).value_counts()/df_pinball_q10.shape[0]\n",
    "print('Frequency models with the highest Pinball Loss 0.1:')\n",
    "print(n_worst)\n",
    "print('sum', n_worst.sum())\n",
    "print(' ')\n",
    "\n",
    "# create dataframe where lowest Pinball is 1 and highest Pinball is -1 and others are 0 per date\n",
    "df_binary = pd.DataFrame(index=df_pinball_q10.index, columns=df_pinball_q10.columns)\n",
    "for date in df_pinball_q10.index:\n",
    "    df_binary.loc[date] = np.where(df_pinball_q10.loc[date] == df_pinball_q10.loc[date].min(), float(1.0), \n",
    "                        np.where(df_pinball_q10.loc[date] == df_pinball_q10.loc[date].max(), float(-1.0), \n",
    "                        float(0.0)))\n",
    "\n",
    "# plot heatmap for df_binary\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df_binary.T.astype(float), cmap='RdYlGn', ax=ax)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('forecasters')\n",
    "plt.title('Best Pinball Loss 0.1 per date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinball loss 0.9\n",
    "list_QRA_over_dayahead_09 = [1 if pinball_loss_QRA <pinball_loss_dayahead else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_dayahead in pinball_loss_09_list_dayahead]\n",
    "list_QRA_over_dayahead11h_09 = [1 if pinball_loss_QRA <pinball_loss_dayahead11h else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_dayahead11h in pinball_loss_09_list_dayahead11h]\n",
    "list_QRA_over_weekahead_09 = [1 if pinball_loss_QRA <pinball_loss_weekahead else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_weekahead in pinball_loss_09_list_weekahead]\n",
    "list_QRA_over_best_09 = [1 if pinball_loss_QRA <pinball_loss_best else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_best in pinball_loss_09_list_best]\n",
    "list_QRA_over_equal_weights_09 = [1 if pinball_loss_QRA <pinball_loss_equal_weights else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_equal_weights in pinball_loss_09_list_equal_weights]\n",
    "list_QRA_over_weight_avg_09 = [1 if pinball_loss_QRA <pinball_loss_weight_avg else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_weight_avg in pinball_loss_09_list_weight_avg]\n",
    "list_QRA_over_weight_avg_soft_09 = [1 if pinball_loss_QRA <pinball_loss_weight_avg_soft else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_weight_avg_soft in pinball_loss_09_list_weight_avg_soft]\n",
    "if sim_params['most_recent']:\n",
    "    list_QRA_over_most_recent_09 = [1 if pinball_loss_QRA <pinball_loss_most_recent else 0 for pinball_loss_QRA in pinball_loss_09_list_QRA for pinball_loss_most_recent in pinball_loss_09_list_most_recent]\n",
    "    \n",
    "# create dataframe\n",
    "start_index = 0\n",
    "end_index = -1\n",
    "df_pinball_q90 = pd.DataFrame({'QRA': pinball_loss_09_list_QRA, 'Day-ahead': pinball_loss_09_list_dayahead, 'Day-ahead 11h': pinball_loss_09_list_dayahead11h, 'Week-ahead': pinball_loss_09_list_weekahead, 'Best Selected': pinball_loss_09_list_best, 'Equal Weights': pinball_loss_09_list_equal_weights, 'Weighted Avg': pinball_loss_09_list_weight_avg, 'Weighted Avg Soft': pinball_loss_09_list_weight_avg_soft})\n",
    "if sim_params['most_recent']:\n",
    "    df_pinball_q90['Most Recent'] = pinball_loss_09_list_most_recent\n",
    "df_pinball_q90 = df_pinball_q90.iloc[start_index:end_index]\n",
    "\n",
    "# frequency QRA has the lowest Pinball over the other models df_pinball_q90\n",
    "n_best = df_pinball_q90.apply(lambda x: x.idxmin(), axis=1).value_counts()/df_pinball_q90.shape[0]\n",
    "print('Frequency models with the lowest Pinball Loss 0.9:')\n",
    "print(n_best)\n",
    "print('sum', n_best.sum())\n",
    "print(' ')\n",
    "# frequency QRA has the highest Pinball over the other models df_pinball_q90\n",
    "n_worst = df_pinball_q90.apply(lambda x: x.idxmax(), axis=1).value_counts()/df_pinball_q90.shape[0]\n",
    "print('Frequency models with the highest Pinball Loss 0.9:')\n",
    "print(n_worst)\n",
    "print('sum', n_worst.sum())\n",
    "\n",
    "# create dataframe where lowest Pinball is 1 and highest Pinball is -1 and others are 0 per date\n",
    "df_binary = pd.DataFrame(index=df_pinball_q90.index, columns=df_pinball_q90.columns)\n",
    "for date in df_pinball_q90.index:\n",
    "    df_binary.loc[date] = np.where(df_pinball_q90.loc[date] == df_pinball_q90.loc[date].min(), float(1.0), \n",
    "                        np.where(df_pinball_q90.loc[date] == df_pinball_q90.loc[date].max(), float(-1.0), \n",
    "                        float(0.0)))\n",
    "start_index = 0\n",
    "end_index = -1\n",
    "df_binary = df_binary.iloc[start_index:end_index]\n",
    "# plot heatmap for df_binary\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "sns.heatmap(df_binary.T.astype(float), cmap='RdYlGn', ax=ax)\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('forecasters')\n",
    "plt.title('Best Pinball Loss 0.9 per date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot barplot\n",
    "# rmse\n",
    "title = 'RMSE by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, rmse_list_QRA, rmse_list_dayahead, rmse_list_dayahead11h, rmse_list_weekahead, rmse_list_best, rmse_list_weight_avg, rmse_list_weight_avg_soft, rmse_list_equal_weights, rmse_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, rmse_list_QRA, rmse_list_dayahead, rmse_list_dayahead11h, rmse_list_weekahead, rmse_list_best, rmse_list_weight_avg, rmse_list_weight_avg_soft, rmse_list_equal_weights)\n",
    "\n",
    "# pinball loss 0.1\n",
    "title = 'Pinball Loss 0.1 by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, pinball_loss_01_list_QRA, pinball_loss_01_list_dayahead, pinball_loss_01_list_dayahead11h, pinball_loss_01_list_weekahead, pinball_loss_01_list_best, pinball_loss_01_list_weight_avg, pinball_loss_01_list_weight_avg_soft, pinball_loss_01_list_equal_weights, pinball_loss_01_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, pinball_loss_01_list_QRA, pinball_loss_01_list_dayahead, pinball_loss_01_list_dayahead11h, pinball_loss_01_list_weekahead, pinball_loss_01_list_best, pinball_loss_01_list_weight_avg, pinball_loss_01_list_weight_avg_soft, pinball_loss_01_list_equal_weights)\n",
    "\n",
    "# pinball loss 0.9\n",
    "title = 'Pinball Loss 0.9 by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, pinball_loss_09_list_QRA, pinball_loss_09_list_dayahead, pinball_loss_09_list_dayahead11h, pinball_loss_09_list_weekahead, pinball_loss_09_list_best, pinball_loss_09_list_weight_avg, pinball_loss_09_list_weight_avg_soft, pinball_loss_09_list_equal_weights, pinball_loss_09_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, pinball_loss_09_list_QRA, pinball_loss_09_list_dayahead, pinball_loss_09_list_dayahead11h, pinball_loss_09_list_weekahead, pinball_loss_09_list_best, pinball_loss_09_list_weight_avg, pinball_loss_09_list_weight_avg_soft, pinball_loss_09_list_equal_weights)\n",
    "\n",
    "# winkler score\n",
    "title = 'Winkler Score by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, winkler_score_list_QRA, winkler_score_list_dayahead, winkler_score_list_dayahead11h, winkler_score_list_weekahead, winkler_score_list_best, winkler_score_list_weight_avg, winkler_score_list_weight_avg_soft, winkler_score_list_equal_weights, winkler_score_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, winkler_score_list_QRA, winkler_score_list_dayahead, winkler_score_list_dayahead11h, winkler_score_list_weekahead, winkler_score_list_best, winkler_score_list_weight_avg, winkler_score_list_weight_avg_soft, winkler_score_list_equal_weights)\n",
    "\n",
    "# coverage\n",
    "title = 'Coverage by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, coverage_list_QRA, coverage_list_dayahead, coverage_list_dayahead11h, coverage_list_weekahead, coverage_list_best, coverage_list_weight_avg, coverage_list_weight_avg_soft, coverage_list_equal_weights, coverage_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, coverage_list_QRA, coverage_list_dayahead, coverage_list_dayahead11h, coverage_list_weekahead, coverage_list_best, coverage_list_weight_avg, coverage_list_weight_avg_soft, coverage_list_equal_weights)\n",
    "\n",
    "# sharpness\n",
    "title = 'Sharpness by forecaster'\n",
    "if sim_params['most_recent']:\n",
    "    barplot(sim_params, title, sharpness_list_QRA, sharpness_list_dayahead, sharpness_list_dayahead11h, sharpness_list_weekahead, sharpness_list_best, sharpness_list_weight_avg, sharpness_list_weight_avg_soft, sharpness_list_equal_weights, sharpness_list_most_recent)\n",
    "else:\n",
    "    barplot(sim_params, title, sharpness_list_QRA, sharpness_list_dayahead, sharpness_list_dayahead11h, sharpness_list_weekahead, sharpness_list_best, sharpness_list_weight_avg, sharpness_list_weight_avg_soft, sharpness_list_equal_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of days:', len(rmse_list_QRA))\n",
    "print('Total number of years:', len(rmse_list_QRA_month_year)/12)\n",
    "print(' ')\n",
    "\n",
    "if sim_params['most_recent']:\n",
    "    df_freq = pd.DataFrame({\n",
    "    'model': ['dayahead', 'dayahead11h', 'weekahead', 'best_model', 'equal_weights', 'weight_avg', 'weight_avg_soft', 'most_recent'],\n",
    "    'rmse': [sum(list_QRA_over_dayahead)/len(list_QRA_over_dayahead), sum(list_QRA_over_dayahead11h)/len(list_QRA_over_dayahead11h), sum(list_QRA_over_weekahead)/len(list_QRA_over_weekahead), sum(list_QRA_over_best)/len(list_QRA_over_best), sum(list_QRA_over_equal_weights)/len(list_QRA_over_equal_weights), sum(list_QRA_over_weight_avg)/len(list_QRA_over_weight_avg), sum(list_QRA_over_weight_avg_soft)/len(list_QRA_over_weight_avg_soft), sum(list_QRA_over_most_recent)/len(list_QRA_over_most_recent)],\n",
    "    'pinball_loss_01': [sum(list_QRA_over_dayahead_01)/len(list_QRA_over_dayahead_01), sum(list_QRA_over_dayahead11h_01)/len(list_QRA_over_dayahead11h_01), sum(list_QRA_over_weekahead_01)/len(list_QRA_over_weekahead_01), sum(list_QRA_over_best_01)/len(list_QRA_over_best_01), sum(list_QRA_over_equal_weights_01)/len(list_QRA_over_equal_weights_01), sum(list_QRA_over_weight_avg_01)/len(list_QRA_over_weight_avg_01), sum(list_QRA_over_weight_avg_soft_01)/len(list_QRA_over_weight_avg_soft_01), sum(list_QRA_over_most_recent_01)/len(list_QRA_over_most_recent_01)],\n",
    "    'pinball_loss_09': [sum(list_QRA_over_dayahead_09)/len(list_QRA_over_dayahead_09), sum(list_QRA_over_dayahead11h_09)/len(list_QRA_over_dayahead11h_09), sum(list_QRA_over_weekahead_09)/len(list_QRA_over_weekahead_09), sum(list_QRA_over_best_09)/len(list_QRA_over_best_09), sum(list_QRA_over_equal_weights_09)/len(list_QRA_over_equal_weights_09), sum(list_QRA_over_weight_avg_09)/len(list_QRA_over_weight_avg_09), sum(list_QRA_over_weight_avg_soft_09)/len(list_QRA_over_weight_avg_soft_09), sum(list_QRA_over_most_recent_09)/len(list_QRA_over_most_recent_09)]\n",
    "})\n",
    "\n",
    "else:\n",
    "    # create dataframe with the frequency results with header 'QRA over model'\n",
    "    df_freq = pd.DataFrame({\n",
    "    'model': ['dayahead', 'dayahead11h', 'weekahead', 'best_model', 'equal_weights', 'weight_avg', 'weight_avg_soft'],\n",
    "    'rmse': [sum(list_QRA_over_dayahead)/len(list_QRA_over_dayahead), sum(list_QRA_over_dayahead11h)/len(list_QRA_over_dayahead11h), sum(list_QRA_over_weekahead)/len(list_QRA_over_weekahead), sum(list_QRA_over_best)/len(list_QRA_over_best), sum(list_QRA_over_equal_weights)/len(list_QRA_over_equal_weights), sum(list_QRA_over_weight_avg)/len(list_QRA_over_weight_avg), sum(list_QRA_over_weight_avg_soft)/len(list_QRA_over_weight_avg_soft)],\n",
    "    'pinball_loss_01': [sum(list_QRA_over_dayahead_01)/len(list_QRA_over_dayahead_01), sum(list_QRA_over_dayahead11h_01)/len(list_QRA_over_dayahead11h_01), sum(list_QRA_over_weekahead_01)/len(list_QRA_over_weekahead_01), sum(list_QRA_over_best_01)/len(list_QRA_over_best_01), sum(list_QRA_over_equal_weights_01)/len(list_QRA_over_equal_weights_01), sum(list_QRA_over_weight_avg_01)/len(list_QRA_over_weight_avg_01), sum(list_QRA_over_weight_avg_soft_01)/len(list_QRA_over_weight_avg_soft_01)],\n",
    "    'pinball_loss_09': [sum(list_QRA_over_dayahead_09)/len(list_QRA_over_dayahead_09), sum(list_QRA_over_dayahead11h_09)/len(list_QRA_over_dayahead11h_09), sum(list_QRA_over_weekahead_09)/len(list_QRA_over_weekahead_09), sum(list_QRA_over_best_09)/len(list_QRA_over_best_09), sum(list_QRA_over_equal_weights_09)/len(list_QRA_over_equal_weights_09), sum(list_QRA_over_weight_avg_09)/len(list_QRA_over_weight_avg_09), sum(list_QRA_over_weight_avg_soft_09)/len(list_QRA_over_weight_avg_soft_09)]\n",
    "})\n",
    "\n",
    "# plot heatmap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_heatmap(df, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.heatmap(df.set_index('model').T, annot=True, cmap='coolwarm', fmt=\".2f\", annot_kws={\"size\": 10})\n",
    "    plt.title(title)\n",
    "    # save figure\n",
    "    plt.savefig(f'{col}_frequency.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "for col in df_freq.columns:\n",
    "    if col != 'model':\n",
    "        plot_heatmap(df_freq[['model', col]], title=f'Frequency QRA outperforms other models for {col}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "\n",
    "def statistical_hypothesis_testing(sim_params, title, loss_list_QRA, loss_list_dayahead, loss_list_dayahead11h, loss_list_weekahead, loss_best_model, loss_weight_avg, loss_weight_avg_soft, loss_equal_weights, loss_most_recent=None):\n",
    "    \"\"\" Perform statistical hypothesis testing using the Friedman test and posthoc Nemenyi test \n",
    "    \"\"\"\n",
    "    # Construct the dictionary from the input lists\n",
    "    dict_data = {\n",
    "        'QRA': loss_list_QRA,\n",
    "        'dayahead': loss_list_dayahead,\n",
    "        'dayahead11h': loss_list_dayahead11h,\n",
    "        'weekahead': loss_list_weekahead,\n",
    "        'selection': loss_best_model,\n",
    "        'weight_avg': loss_weight_avg,\n",
    "        'weight_avg_soft': loss_weight_avg_soft,\n",
    "        'weight_equal': loss_equal_weights\n",
    "    }\n",
    "    if sim_params['most_recent']:\n",
    "        dict_data['most_recent'] = loss_most_recent\n",
    "\n",
    "    # Transform the dictionary into a DataFrame\n",
    "    data = (\n",
    "        pd.DataFrame(dict_data)\n",
    "        .rename_axis('days')  # Set the index name to 'days'\n",
    "        .melt(                # Melt the DataFrame to long format\n",
    "            var_name='model',\n",
    "            value_name='loss',\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        .reset_index()        # Reset the index to include 'days' as a column\n",
    "    )\n",
    "    # Perform posthoc Nemenyi Friedman test\n",
    "    tests_results = sp.posthoc_nemenyi_friedman(data, y_col='loss', group_col='model', block_col='days',  block_id_col='days', melted=True) #\n",
    "    # Define the colormap and heatmap arguments\n",
    "    cmap = ['1', '#fb6a4a',  '#08306b',  '#4292c6', '#c6dbef']\n",
    "    heatmap_args = {\n",
    "        'cmap': cmap,\n",
    "        'linewidths': 0.25,\n",
    "        'linecolor': '0.5',\n",
    "        'clip_on': False,\n",
    "        'square': True,\n",
    "        'cbar_ax_bbox': [0.80, 0.35, 0.04, 0.3]\n",
    "    }\n",
    "    \n",
    "    # Compute the average rank of each model\n",
    "    print('------------------------------------')\n",
    "    avg_rank = data.groupby('days').loss.rank(method='average').groupby(data.model).mean()\n",
    "    print(f\"Average rank of the models - {avg_rank}\")\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.title(f\"P-values posthoc Nemenyi test for {title}\")\n",
    "    # mask the upper triangle\n",
    "    mask = np.triu(np.ones_like(tests_results, dtype=bool))\n",
    "    tests_results_masked = tests_results.mask(mask)\n",
    "    # print the p-values\n",
    "    # add to column names the avg rank in square brackets\n",
    "    # in bold if QRA\n",
    "    tests_results_masked.columns = [f\"{col} - {avg_rank[col]:.3f}\" for col in tests_results_masked.columns]\n",
    "    tests_results_masked.columns = [f\"**{col}**\" if 'QRA' in col else col for col in tests_results_masked.columns]\n",
    "    tests_results_masked.index = [f\"{col} - {avg_rank[col]:.3f}\" for col in tests_results_masked.index]\n",
    "    tests_results_masked.index = [f\"**{col}**\" if 'QRA' in col else col for col in tests_results_masked.index]\n",
    "    # add header\n",
    "    tests_results_masked.columns.name = 'Average Rank of the models'\n",
    "    tests_results_masked.index.name = 'Average Rank of the models'\n",
    "\n",
    "    # figure size\n",
    "    sp.sign_plot(tests_results_masked, **heatmap_args)\n",
    "    # save image\n",
    "    # split the title\n",
    "    title = title.replace(' ', '_')\n",
    "    plt.savefig(f'p_values_posthoc_nemenyi_test_{title}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse\n",
    "if sim_params['most_recent']:\n",
    "    title = 'RMSE'\n",
    "    statistical_hypothesis_testing(sim_params, title, rmse_list_QRA, rmse_list_dayahead, rmse_list_dayahead11h, rmse_list_weekahead, rmse_list_best, rmse_list_weight_avg, rmse_list_weight_avg_soft, rmse_list_equal_weights, rmse_list_most_recent)\n",
    "else:\n",
    "    title = 'RMSE'\n",
    "    statistical_hypothesis_testing(sim_params, title, rmse_list_QRA, rmse_list_dayahead, rmse_list_dayahead11h, rmse_list_weekahead, rmse_list_best, rmse_list_weight_avg, rmse_list_weight_avg_soft, rmse_list_equal_weights)\n",
    "\n",
    "# pinball loss 0.1\n",
    "if sim_params['most_recent']:\n",
    "    title = 'Pinball Loss 0.1'\n",
    "    statistical_hypothesis_testing(sim_params, title, pinball_loss_01_list_QRA, pinball_loss_01_list_dayahead, pinball_loss_01_list_dayahead11h, pinball_loss_01_list_weekahead, pinball_loss_01_list_best, pinball_loss_01_list_weight_avg, pinball_loss_01_list_weight_avg_soft, pinball_loss_01_list_equal_weights, pinball_loss_01_list_most_recent)\n",
    "else:\n",
    "    title = 'Pinball Loss 0.1'\n",
    "    statistical_hypothesis_testing(sim_params, title, pinball_loss_01_list_QRA, pinball_loss_01_list_dayahead, pinball_loss_01_list_dayahead11h, pinball_loss_01_list_weekahead, pinball_loss_01_list_best, pinball_loss_01_list_weight_avg, pinball_loss_01_list_weight_avg_soft, pinball_loss_01_list_equal_weights)\n",
    "\n",
    "# pinball loss 0.9\n",
    "if sim_params['most_recent']:\n",
    "    title = 'Pinball Loss 0.9'\n",
    "    statistical_hypothesis_testing(sim_params, title, pinball_loss_09_list_QRA, pinball_loss_09_list_dayahead, pinball_loss_09_list_dayahead11h, pinball_loss_09_list_weekahead, pinball_loss_09_list_best, pinball_loss_09_list_weight_avg, pinball_loss_09_list_weight_avg_soft, pinball_loss_09_list_equal_weights, pinball_loss_09_list_most_recent)\n",
    "else:\n",
    "    title = 'Pinball Loss 0.9'\n",
    "    statistical_hypothesis_testing(sim_params, title, pinball_loss_09_list_QRA, pinball_loss_09_list_dayahead, pinball_loss_09_list_dayahead11h, pinball_loss_09_list_weekahead, pinball_loss_09_list_best, pinball_loss_09_list_weight_avg, pinball_loss_09_list_weight_avg_soft, pinball_loss_09_list_equal_weights)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predico-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
