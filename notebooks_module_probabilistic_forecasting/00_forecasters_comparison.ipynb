{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from sklearn.utils.fixes import parse_version, sp_version\n",
    "solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\"\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from source.utils.file_read import read_csv_file, filter_data, join_dataframes, replace_nan_values\n",
    "from source.utils.collect_results import create_df_forecaster_first_stage\n",
    "from source.utils.generate_timestamp import generate_timestamps\n",
    "from source.simulation.submission_module import submission_forecasters\n",
    "from source.simulation.buyer_module import prepare_buyer_data\n",
    "from source.ensemble.combination_scheme.equal_weights import calculate_equal_weights\n",
    "from source.ensemble.combination_scheme.avg_weights import calculate_weighted_avg\n",
    "from source.ensemble.combination_scheme.model_selection import run_model_selection\n",
    "from source.plots.plot_forecasts import plot_forecasts\n",
    "from source.ml_engine import create_ensemble_forecasts\n",
    "from source.simulation.helpers_simulation import process_combination_scheme\n",
    "from source.utils.session_ml_info import delete_previous_day_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration settings\n",
    "from config.simulation_setting import Simulation, WeightedAvg, Stack\n",
    "\n",
    "sim_params = Simulation.testing_period  # Simulation parameters\n",
    "weight_avg_params = WeightedAvg.params  # Weighted Average parameters\n",
    "ens_params = Stack.params  # QRA Ensemble parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Days:  54%|█████▍    | 1089/2000 [1:04:43<38:54,  2.56s/it]\u001b[32m2024-12-15 14:17:50.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.generate_timestamp\u001b[0m:\u001b[36mgenerate_timestamps\u001b[0m:\u001b[36m12\u001b[0m - \u001b[1m \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.generate_timestamp\u001b[0m:\u001b[36mgenerate_timestamps\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1m\u001b[34m-------------------------------------------------------------------------------------------\u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.generate_timestamp\u001b[0m:\u001b[36mgenerate_timestamps\u001b[0m:\u001b[36m14\u001b[0m - \u001b[1m\u001b[34mStart training: 2023-12-26 00:00:00+00:00 - End training: 2024-01-25 00:00:00+00:00\u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.generate_timestamp\u001b[0m:\u001b[36mgenerate_timestamps\u001b[0m:\u001b[36m15\u001b[0m - \u001b[1m\u001b[34m-------------------------------------------------------------------------------------------\u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.generate_timestamp\u001b[0m:\u001b[36mgenerate_timestamps\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1m\u001b[34mStart prediction: 2024-01-26 00:00:00+00:00 - End prediction: 2024-01-27 00:00:00+00:00\u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.448\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.file_read\u001b[0m:\u001b[36mfilter_data\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1m\u001b[34m -----------------> Length of training data: 2880 \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.450\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.file_read\u001b[0m:\u001b[36mfilter_data\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1m\u001b[34m -----------------> Length of testing data: 95 \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.450\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[34m\u001b[1mForecasters submission ...\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.simulation.submission_module\u001b[0m:\u001b[36msubmission_forecasters\u001b[0m:\u001b[36m173\u001b[0m - \u001b[1m \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.679\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.simulation.submission_module\u001b[0m:\u001b[36msubmission_forecasters\u001b[0m:\u001b[36m174\u001b[0m - \u001b[1m\u001b[34m -----------------> Forecasters prediction submitted \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.680\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m39\u001b[0m - \u001b[34m\u001b[1mMarket operator data ...\u001b[0m\n",
      "/Users/gio/Desktop/Elia-RES-Forecasting/source/simulation/buyer_module.py:22: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_buyer = pd.concat([df_train_buyer, df_test_buyer], axis=0)\n",
      "\u001b[32m2024-12-15 14:17:50.681\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m46\u001b[0m - \u001b[34m\u001b[1mWind Ensemble forecasts ...\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.682\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1m  \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m PREDICO Machine Learning Engine \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1m  \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Launch Time from 2024-01-25 00:00:00+00:00 \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.683\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m71\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Predictions from 2024-01-26 00:00:00+00:00 to 2024-01-27 00:00:00+00:00 \u001b[0m\u001b[1m \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1m  \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.684\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Buyer Resource Name: b1r1 \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.data_preprocess\u001b[0m:\u001b[36mbuyer_scaler_statistics\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Mean Buyer: 1426.0804375 \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.data_preprocess\u001b[0m:\u001b[36mbuyer_scaler_statistics\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Std Buyer: 661.28863903266 \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.685\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.data_preprocess\u001b[0m:\u001b[36mbuyer_scaler_statistics\u001b[0m:\u001b[36m165\u001b[0m - \u001b[1m  \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m85\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Collecting forecasters prediction for ensemble learning - model: LR \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m86\u001b[0m - \u001b[1m  \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.686\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m87\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Forecasters Ensemble DataFrame \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.data_preprocess\u001b[0m:\u001b[36mscale_forecasters_dataframe\u001b[0m:\u001b[36m206\u001b[0m - \u001b[1m   \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.687\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.utils.data_preprocess\u001b[0m:\u001b[36mscale_forecasters_dataframe\u001b[0m:\u001b[36m207\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Standardize DataFrame \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m93\u001b[0m - \u001b[1m   \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.689\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Augment DataFrame \u001b[0m\u001b[1m\u001b[0m\n",
      "/Users/gio/Desktop/Elia-RES-Forecasting/source/ensemble/stack_generalization/feature_engineering/data_augmentation.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train.bfill(inplace=True)\n",
      "/Users/gio/Desktop/Elia-RES-Forecasting/source/ensemble/stack_generalization/feature_engineering/data_augmentation.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test.bfill(inplace=True)\n",
      "/Users/gio/Desktop/Elia-RES-Forecasting/source/ensemble/stack_generalization/data_preparation/data_train_test.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train_ensemble.loc[:, 'norm_targ'] = df_train[col_name_buyer].values[max_lag:]\n",
      "/Users/gio/Desktop/Elia-RES-Forecasting/source/ensemble/stack_generalization/data_preparation/data_train_test.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test_ensemble.loc[:, 'norm_targ'] = df_test[col_name_buyer].values\n",
      "\u001b[32m2024-12-15 14:17:50.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m159\u001b[0m - \u001b[1m   \u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m160\u001b[0m - \u001b[1m\u001b[38;2;250;128;114m Train and Test Dataframes \u001b[0m\u001b[1m\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m161\u001b[0m - \u001b[1mLength of Train DataFrame: 2878\u001b[0m\n",
      "\u001b[32m2024-12-15 14:17:50.703\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msource.ml_engine\u001b[0m:\u001b[36mcreate_ensemble_forecasts\u001b[0m:\u001b[36m162\u001b[0m - \u001b[1mLength of Test DataFrame: 95\u001b[0m\n",
      "Testing Days:  54%|█████▍    | 1089/2000 [1:04:44<54:09,  3.57s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Test dataframe must have 96 rows",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 47\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# # ----------------------------> PREDICO PLATFORM ML ENGINE <----------------------------\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# # ----------------------------> ENSEMBLE FORECASTS <----------------------------\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWind Ensemble forecasts ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     results_ensemble_forecasts \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ensemble_forecasts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mens_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mens_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mdf_buyer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_buyer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mdf_market\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_market\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mend_training_timestamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_training_timestamp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mforecast_range\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mforecast_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43mchallenge_usecase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msimulation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                                            \u001b[49m\u001b[43msimulation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m## ----------------------------> SAVE to CSV <----------------------------\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# wind power\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     df_pred_ensemble \u001b[38;5;241m=\u001b[39m results_ensemble_forecasts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwind_power\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Elia-RES-Forecasting/source/ml_engine.py:163\u001b[0m, in \u001b[0;36mcreate_ensemble_forecasts\u001b[0;34m(ens_params, df_buyer, df_market, end_training_timestamp, forecast_range, challenge_usecase, simulation)\u001b[0m\n\u001b[1;32m    161\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of Train DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_train_ensemble)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    162\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of Test DataFrame: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_test_ensemble)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_test_ensemble) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m96\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest dataframe must have 96 rows\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# # Split train and test dataframes quantile predictions\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ens_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madd_quantile_predictions\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Test dataframe must have 96 rows"
     ]
    }
   ],
   "source": [
    "# set random seed\n",
    "np.random.seed(sim_params['random_seed'])\n",
    "\n",
    "# read csv file\n",
    "df_processed = read_csv_file(sim_params['csv_filename'], sim_params['list_columns'], sim_params['starting_period'], sim_params['ending_period'])\n",
    "\n",
    "# replace NaN values\n",
    "if sim_params['replace_nan']:\n",
    "    df_processed = replace_nan_values(sim_params, df_processed)\n",
    "\n",
    "# remove previous day pickle file\n",
    "logger.info(' ')\n",
    "delete_previous_day_pickle()\n",
    "\n",
    "# save csv variables\n",
    "list_csv_to_save = []\n",
    "\n",
    "# loop over test days\n",
    "for i in tqdm(range(sim_params['num_test_days']), desc='Testing Days'):\n",
    "\n",
    "    # generate timestamps train and prediction\n",
    "    start_training_timestamp, end_training_timestamp, start_prediction_timestamp, end_prediction_timestamp = generate_timestamps(sim_params['start_training'], i, sim_params['window_size'])\n",
    "\n",
    "    if i >= ens_params['day_calibration'] and ens_params['conformalized_qr']:\n",
    "        day_calibration = ens_params['day_calibration']\n",
    "        start_training_timestamp = start_training_timestamp - pd.Timedelta(f'{day_calibration}day')\n",
    "\n",
    "    # trimming data for training and testing\n",
    "    df_train = filter_data(df_processed, start_training_timestamp, end_training_timestamp, string = 'training')\n",
    "    df_test = filter_data(df_processed, start_prediction_timestamp, end_prediction_timestamp, string = 'testing')\n",
    "\n",
    "# # ----------------------------> FORECASTERS SUBMISSION <----------------------------\n",
    "\n",
    "    logger.debug(\"Forecasters submission ...\")\n",
    "    df_market, df_train, df_test = submission_forecasters(sim_params, df_train, df_test)   \n",
    "\n",
    "# # ----------------------------> MARKET OPERATOR DATA <----------------------------\n",
    "\n",
    "    logger.debug(\"Market operator data ...\")\n",
    "    df_buyer, forecast_range = prepare_buyer_data(df_train, df_test, start_prediction_timestamp, end_prediction_timestamp)\n",
    "\n",
    "# # ----------------------------> PREDICO PLATFORM ML ENGINE <----------------------------\n",
    "\n",
    "# # ----------------------------> ENSEMBLE FORECASTS <----------------------------\n",
    "\n",
    "    logger.debug(\"Wind Ensemble forecasts ...\")\n",
    "    results_ensemble_forecasts = create_ensemble_forecasts(ens_params=ens_params,\n",
    "                                                            df_buyer=df_buyer, \n",
    "                                                            df_market=df_market,\n",
    "                                                            end_training_timestamp=end_training_timestamp,\n",
    "                                                            forecast_range = forecast_range,\n",
    "                                                            challenge_usecase='simulation',\n",
    "                                                            simulation=True)\n",
    "    \n",
    "    ## ----------------------------> SAVE to CSV <----------------------------\n",
    "    # wind power\n",
    "    df_pred_ensemble = results_ensemble_forecasts['wind_power']['predictions']\n",
    "    df_pred_ensemble.rename(columns={'q50_' + sim_params['buyer_resource_name']: '50_predictions', \n",
    "                                        'q10_' + sim_params['buyer_resource_name']: '10_predictions',\n",
    "                                        'q90_' + sim_params['buyer_resource_name']: '90_predictions', \n",
    "                                        'norm_' + sim_params['buyer_resource_name']: 'targets'}, inplace=True)\n",
    "    # create dataframes\n",
    "    df_test_ensemble = pd.DataFrame(df_test['measured']) \n",
    "    df_test_ensemble.rename(columns={'measured': 'targets'}, inplace=True)\n",
    "    \n",
    "    # drop targets column\n",
    "    df_pred_ensemble_clean = df_pred_ensemble.drop(columns=['targets'], axis=1)\n",
    "\n",
    "    # list dataframes wind power\n",
    "    list_df_wind_power = [df_test, df_pred_ensemble_clean]\n",
    "\n",
    "    if sim_params['baselines_comparison']:\n",
    "\n",
    "        # # # ----------------------------> COMBINATION SCHEME DATA <----------------------------\n",
    "\n",
    "        # process data for baselines combination schemes\n",
    "        logger.debug(\"Combination scheme data ...\")\n",
    "        df_train_norm, day_previous_df_test_norm, day_previous_df_test_norm_var = process_combination_scheme(df_train, df_test, end_training_timestamp, start_prediction_timestamp)\n",
    "        \n",
    "        # # ----------------------------> PERFORMANCE METRICS <----------------------------\n",
    "\n",
    "        ## ----------------------------> WIND POWER <----------------------------\n",
    "\n",
    "        # performance best model selection\n",
    "        logger.debug(\"Best model selection ...\")\n",
    "        df_best_model = run_model_selection(sim_params, df_train_norm, day_previous_df_test_norm, end_training_timestamp, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'])\n",
    "        df_best_model_clean = df_best_model.drop(columns=['targets'], axis=1)\n",
    "        list_df_wind_power.append(df_best_model_clean)\n",
    "        \n",
    "        # performance weighted average\n",
    "        logger.debug(\"Weighted average ...\")\n",
    "        df_weighted_avg, dict_weights = calculate_weighted_avg(sim_params, \n",
    "                                                            df_train_norm, \n",
    "                                                            day_previous_df_test_norm, \n",
    "                                                            end_training_timestamp, \n",
    "                                                            start_prediction_timestamp, \n",
    "                                                            window_size_valid=weight_avg_params['window_size_valid'])\n",
    "        df_weighted_avg_clean = df_weighted_avg.drop(columns=['targets'], axis=1)\n",
    "        list_df_wind_power.append(df_weighted_avg_clean)\n",
    "\n",
    "        # performance weighted avg soft\n",
    "        logger.debug(\"Weighted average soft ...\")\n",
    "        df_weighted_avg_soft, dict_weights_soft = calculate_weighted_avg(sim_params, \n",
    "                                                                        df_train_norm, \n",
    "                                                                        day_previous_df_test_norm, \n",
    "                                                                        end_training_timestamp, \n",
    "                                                                        start_prediction_timestamp, \n",
    "                                                                        window_size_valid=weight_avg_params['window_size_valid'], \n",
    "                                                                        norm='softmax')\n",
    "        df_weighted_avg_soft_clean = df_weighted_avg_soft.drop(columns=['targets'], axis=1)\n",
    "        list_df_wind_power.append(df_weighted_avg_soft_clean)\n",
    "        \n",
    "        # performance equal weights\n",
    "        logger.debug(\"Equal weights ...\")\n",
    "        df_equal_weights = calculate_equal_weights(day_previous_df_test_norm, start_prediction_timestamp)\n",
    "        df_equal_weights_clean = df_equal_weights.drop(columns=['targets'], axis=1)\n",
    "        list_df_wind_power.append(df_equal_weights_clean)\n",
    "\n",
    "        # performance malicious cheat\n",
    "        if sim_params['malicious']:\n",
    "            logger.debug(\"Malicious forecaster ...\")\n",
    "            df_malicious = create_df_forecaster_first_stage(day_previous_df_test_norm, 'malicious', start_prediction_timestamp)\n",
    "            list_df_wind_power.append(df_malicious)\n",
    "\n",
    "        # performance noisy\n",
    "        if sim_params['noisy']:\n",
    "            logger.debug(\"Noisy forecaster ...\")\n",
    "            df_noisy = create_df_forecaster_first_stage(day_previous_df_test_norm, 'noisy', start_prediction_timestamp)\n",
    "            list_df_wind_power.append(df_noisy)\n",
    "\n",
    "        # plot forecasts\n",
    "        if ens_params['plt_wind_power_ensemble']:\n",
    "            plot_forecasts(df_pred_ensemble, df_test_ensemble, list_wind_ramps=[], title=f'Wind Power Forecasting')\n",
    "\n",
    "    # join dataframes wind power forecasters baseline\n",
    "    df_csv_wind_power = join_dataframes(*list_df_wind_power)\n",
    "    list_csv_to_save.append(df_csv_wind_power)\n",
    "\n",
    "    #Clear output\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # import time\n",
    "    # time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------> SAVE TO CSV <----------------------------\n",
    "# from list to dataframe\n",
    "if sim_params['most_recent']:\n",
    "    df_csv = pd.concat(list_csv_to_save).to_csv('wp_forecasters_comparison_results.csv')\n",
    "else:\n",
    "    df_csv = pd.concat(list_csv_to_save).to_csv('wp_forecasters_comparison_results_no_mostrecent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------> PLOT FORECASTS <----------------------------\n",
    "df_csv[['measured', '10_predictions', '50_predictions', '90_predictions']].iloc[:2000].plot(figsize=(20,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predico-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
