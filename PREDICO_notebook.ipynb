{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "from source.utils.file_read import process_and_concat_files \n",
    "from source.utils.file_read import filter_df\n",
    "from source.utils.generate_timestamp import generate_timestamps\n",
    "from source.forecasters.deterministic import create_day_ahead_predictions, create_day_ahead_11_predictions, create_week_ahead_predictions\n",
    "from source.forecasters.probabilistic import create_day_ahead_quantiles10, create_day_ahead_11_quantiles10, create_week_ahead_quantiles10\n",
    "from source.forecasters.probabilistic import create_day_ahead_quantiles90, create_day_ahead_11_quantiles90, create_week_ahead_quantiles90\n",
    "from source.utils.data_preprocess import normalize_dataframe\n",
    "from source.ensemble.stack_generalization.feature_engineering.data_augmentation import create_augmented_dataframe\n",
    "from source.ensemble.stack_generalization.data_preparation.data_train_test import prepare_train_test_data, get_numpy_Xy_train_test\n",
    "from source.ensemble.stack_generalization.ensemble_model import run_ensemble_predictions_per_quantile, run_ensemble_variability_predictions\n",
    "from source.ensemble.stack_generalization.second_stage.create_data_second_stage import create_2stage_dataframe, create_augmented_dataframe_2stage, create_var_ensemble_dataframe\n",
    "from source.ensemble.stack_generalization.utils.results import collect_quantile_ensemble_predictions, create_ensemble_dataframe\n",
    "from source.ensemble.stack_generalization.utils.display_results import display_forecasting_metrics\n",
    "from source.ensemble.combination_scheme.equal_weights import calculate_equal_weights\n",
    "from source.ensemble.combination_scheme.avg_weights import calculate_weighted_avg\n",
    "from source.ensemble.utils.metrics import calculate_rmse, calculate_pinball_losses\n",
    "from source.plots.plot_forecasts import plot_ensemble_forecasts, plot_var_ensemble_forecasts, plot_ramp_events, plot_weighted_avg_forecasts, plot_baseline_forecasts\n",
    "from source.ensemble.stack_generalization.test_importance.first_stage_importance import first_stage_permutation_importance\n",
    "from source.ensemble.stack_generalization.test_importance.second_stage_importance import second_stage_permutation_importance\n",
    "from source.ensemble.stack_generalization.test_importance.plot_permutation_importance import plot_normalized_contributions\n",
    "from source.ensemble.combination_scheme.weight_avg_plot_importance import plot_weight_avg_contributions\n",
    "from sklearn.utils.fixes import parse_version, sp_version\n",
    "solver = \"highs\" if sp_version >= parse_version(\"1.6.0\") else \"interior-point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config.PREDICO_setting import Simulation, WeightedAvg, Stack\n",
    "\n",
    "sim_params = Simulation.testing_period\n",
    "weight_avg_params = WeightedAvg.params\n",
    "ens_params = Stack.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "files = [sim_params['file_0'], sim_params['file_1'], sim_params['file_2'], sim_params['file_3'], sim_params['file_4']]\n",
    "df = process_and_concat_files(files)\n",
    "\n",
    "# get the maximum capacity\n",
    "maximum_capacity = df.measured.max()\n",
    "\n",
    "df_filtered = filter_df(df, sim_params['forecasts_col'], sim_params['measured_col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss quantile gradient boosting regressor\n",
    "lst_rmse_gbr_ensemble = []\n",
    "# loss equal weights scheme\n",
    "lst_rmse_equal_weights = []\n",
    "# loss weighted average scheme\n",
    "lst_rmse_weighted_avg = []\n",
    "# loss baseline day ahead\n",
    "lst_rmse_baseline_dayahead = []\n",
    "# loss baseline day ahead 11\n",
    "lst_rmse_baseline_dayahead11h = []\n",
    "# loss baseline week ahead\n",
    "lst_rmse_baseline_week_ahead = []\n",
    "\n",
    "# loss var gradient boosting regressor\n",
    "lst_rmse_var_gbr_ensemble = []\n",
    "# loss var equal weights scheme\n",
    "lst_rmse_var_equal_weights = []\n",
    "# loss var weighted average scheme\n",
    "lst_rmse_var_weighted_avg = []\n",
    "# loss var baseline day ahead\n",
    "lst_rmse_var_baseline_dayahead = []\n",
    "# loss var baseline day ahead 11\n",
    "lst_rmse_var_baseline_dayahead11h = []\n",
    "# loss var baseline week ahead\n",
    "lst_rmse_var_baseline_week_ahead = []\n",
    "\n",
    "# loss quantile gradient boosting regressor\n",
    "lst_pb_gbr_ensemble_q10 = []\n",
    "lst_pb_gbr_ensemble_q90 = []\n",
    "# loss equal weights scheme\n",
    "lst_pb_weighted_avg_q10 = []\n",
    "lst_pb_weighted_avg_q90 = []\n",
    "# loss weighted average scheme\n",
    "lst_pb_equal_weights_q10 = []\n",
    "lst_pb_equal_weights_q90 = []\n",
    "# loss baseline day ahead\n",
    "lst_pb_dayahead_q10 = []\n",
    "lst_pb_dayahead_q90  = []\n",
    "# loss baseline day ahead 11\n",
    "lst_pb_dayahead_11h_q10 = []\n",
    "lst_pb_dayahead_11h_q90 = []\n",
    "# loss baseline week ahead\n",
    "lst_pb_week_ahead_q10 = []\n",
    "lst_pb_week_ahead_q90 = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "# generate timestamps train and prediction\n",
    "start_training_timestamp, end_training_timestamp, start_prediction_timestamp, end_prediction_timestamp = generate_timestamps(sim_params['start_training'], i, sim_params['window_size'])\n",
    "\n",
    "logger.info(' ')\n",
    "logger.opt(colors = True).info('<blue>-------------------------------------------------------------------------------------------</blue>')\n",
    "logger.opt(colors = True).info(f'<blue>Start prediction: {start_prediction_timestamp} - End prediction: {end_prediction_timestamp}</blue>')\n",
    "\n",
    "df_train = df_filtered[df_filtered.index.to_series().between(start_training_timestamp, end_training_timestamp)].iloc[:-1,:]\n",
    "df_test = df_filtered[df_filtered.index.to_series().between(start_prediction_timestamp, end_prediction_timestamp)].iloc[:-1,:]\n",
    "\n",
    "logger.info(' ')\n",
    "logger.opt(colors = True).info('<blue> -----------------> Forecasters prediction submitted </blue>')\n",
    "\n",
    "# forecaster - day ahead forecast\n",
    "df_day_ahead_pred_train = create_day_ahead_predictions(df_train)\n",
    "df_day_ahead_pred_test = create_day_ahead_predictions(df_test)\n",
    "\n",
    "# forecaster - day ahead 11 forecast\n",
    "df_day_ahead11_pred_train = create_day_ahead_11_predictions(df_train)\n",
    "df_day_ahead11_pred_test = create_day_ahead_11_predictions(df_test)\n",
    "\n",
    "# forecaster - week ahead forecast\n",
    "df_week_ahead_pred_train = create_week_ahead_predictions(df_train)\n",
    "df_week_ahead_pred_test = create_week_ahead_predictions(df_test)\n",
    "\n",
    "# forecaster - day ahead quantile-10\n",
    "df_day_ahead_q10_train = create_day_ahead_quantiles10(df_train)\n",
    "df_day_ahead_q10_test = create_day_ahead_quantiles10(df_test)\n",
    "\n",
    "# forecaster - day ahead 11 quantile-10\n",
    "df_day_ahead11_q10_train = create_day_ahead_11_quantiles10(df_train)\n",
    "df_day_ahead11_q10_test = create_day_ahead_11_quantiles10(df_test)\n",
    "\n",
    "# forecaster - week ahead quantile-10\n",
    "df_week_ahead_q10_train = create_week_ahead_quantiles10(df_train)\n",
    "df_week_ahead_q10_test = create_week_ahead_quantiles10(df_test)\n",
    "\n",
    "# forecaster - day ahead quantile-90\n",
    "df_day_ahead_q90_train = create_day_ahead_quantiles90(df_train)\n",
    "df_day_ahead_q90_test = create_day_ahead_quantiles90(df_test)\n",
    "\n",
    "# forecaster - day ahead 11 quantile-90\n",
    "df_day_ahead11_q90_train = create_day_ahead_11_quantiles90(df_train)\n",
    "df_day_ahead11_q90_test = create_day_ahead_11_quantiles90(df_test)\n",
    "\n",
    "# forecaster - week ahead quantile-90\n",
    "df_week_ahead_q90_train = create_week_ahead_quantiles90(df_train)\n",
    "df_week_ahead_q90_test = create_week_ahead_quantiles90(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICO ML engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_first_stage_importance(ens_params, previous_day_results_first_stage, y_test):\n",
    "    \" Process the importance of the first stage \"\n",
    "    dict_results_df = {}\n",
    "    if ens_params['plot_importance_permutation_first_stage']:\n",
    "        for quantile in ens_params['quantiles']:\n",
    "                previous_day_results = previous_day_results_first_stage[quantile]\n",
    "                logger.info('   ')\n",
    "                logger.opt(colors=True).info(f'<fg 250,128,114> EX-Post Payments with loss-based importance </fg 250,128,114>')\n",
    "                resultsdf = first_stage_permutation_importance(ens_params['nr_permutations'], \n",
    "                                                            quantile, \n",
    "                                                            previous_day_results['fitted_model'], \n",
    "                                                            previous_day_results['X_test_augmented'], \n",
    "                                                            y_test, \n",
    "                                                            previous_day_results['df_train_ensemble_augmented'])\n",
    "                plot_normalized_contributions(ens_params['nr_permutations'], quantile, resultsdf, stage='Wind Power')\n",
    "                logger.info('   ')\n",
    "                dict_results_df[quantile] = resultsdf\n",
    "    return dict_results_df\n",
    "\n",
    "\n",
    "def process_second_stage_importance(ens_params, previous_day_results_second_stage, y_train, y_test):\n",
    "    \" Process the importance of the second stage \"\n",
    "    dict_results_df = {}\n",
    "    if ens_params['plot_importance_permutation_second_stage']:\n",
    "        for quantile in ens_params['quantiles']:\n",
    "            previous_day_results = previous_day_results_second_stage[quantile]\n",
    "            logger.info('   ')\n",
    "            logger.opt(colors=True).info(f'<fg 250,128,114> EX-Post Payments with loss-based importance </fg 250,128,114>')\n",
    "            resultsdf = second_stage_permutation_importance(ens_params['nr_permutations'],\n",
    "                                                            quantile, \n",
    "                                                            previous_day_results['var_fitted_model'], \n",
    "                                                            previous_day_results['fitted_model'], \n",
    "                                                            previous_day_results['X_test_augmented'], \n",
    "                                                            y_test, \n",
    "                                                            previous_day_results['df_train_ensemble_augmented'],\n",
    "                                                            previous_day_results['X_train_augmented'], \n",
    "                                                            previous_day_results['df_train_ensemble'], \n",
    "                                                            previous_day_results['df_test_ensemble'], \n",
    "                                                            y_train, \n",
    "                                                            ens_params['order_diff'], \n",
    "                                                            ens_params['max_lags_var'], \n",
    "                                                            ens_params['augment_var'], \n",
    "                                                            start_prediction_timestamp)\n",
    "            plot_normalized_contributions(ens_params['nr_permutations'], \n",
    "                                        quantile, resultsdf, stage='Wind Power Variability')\n",
    "            logger.info('   ')\n",
    "            dict_results_df[quantile] = resultsdf\n",
    "    return dict_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_day_results_first_stage = {}  # collect results for forecasting contribution assessment first stage\n",
    "previous_day_results_second_stage = {}  # collect results for forecasting contribution assessment second stage\n",
    "y_test = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML ENGINE PREDICO PLATFORM\n",
    "logger.info('  ')\n",
    "logger.opt(colors = True).info(f'<fg 250,128,114> PREDICO PLATFORM ML ENGINE </fg 250,128,114> ')\n",
    "logger.info('  ')\n",
    "\n",
    "\n",
    "# Compute Forecasters' Contribution\n",
    "logger.info(\"--\" * 79)\n",
    "if previous_day_results_first_stage:\n",
    "    logger.opt(colors = True).info(f\"<fg 250,128,114> Compute Forecasters' Contribution First Stage </fg 250,128,114>\")\n",
    "    dict_results_df_first_stage = process_first_stage_importance(ens_params, previous_day_results_first_stage, y_test)\n",
    "logger.info(\"--\" * 79)           \n",
    "if previous_day_results_second_stage:\n",
    "    logger.opt(colors = True).info(f\"<fg 250,128,114> Compute Forecasters' Contribution Second Stage </fg 250,128,114>\")\n",
    "    dict_results_df_first_stage = process_second_stage_importance(ens_params, previous_day_results_second_stage, y_train, y_test)\n",
    "logger.info(\"--\" * 79)\n",
    "logger.info('  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect Forecasters Prediction for Ensemble Learning\n",
    "logger.opt(colors = True).info(f'<fg 250,128,114> Collecting forecasters prediction for ensemble learning - model: {ens_params[\"model_type\"]} </fg 250,128,114> ')\n",
    "logger.info('  ')\n",
    "logger.opt(colors=True).info(f'<fg 250,128,114> Forecasters Ensemble DataFrame </fg 250,128,114>')\n",
    "# make esemble dataframe \n",
    "df_train_ensemble = pd.concat([df_day_ahead_pred_train, df_day_ahead11_pred_train, df_week_ahead_pred_train], axis=1) \n",
    "df_test_ensemble = pd.concat([df_day_ahead_pred_test, df_day_ahead11_pred_test, df_week_ahead_pred_test], axis=1)\n",
    "df_ensemble = pd.concat([df_train_ensemble, df_test_ensemble], axis=0)\n",
    "\n",
    "if ens_params['add_quantile_predictions']:\n",
    "    df_train_ensemble_quantile10 = pd.concat([df_day_ahead_q10_train, df_day_ahead11_q10_train, df_week_ahead_q10_train], axis=1)\n",
    "    df_train_ensemble_quantile90 = pd.concat([df_day_ahead_q90_train, df_day_ahead11_q90_train, df_week_ahead_q90_train], axis=1)\n",
    "    df_test_ensemble_quantile10 = pd.concat([df_day_ahead_q10_test, df_day_ahead11_q10_test, df_week_ahead_q10_test], axis=1)\n",
    "    df_test_ensemble_quantile90 = pd.concat([df_day_ahead_q90_test, df_day_ahead11_q90_test, df_week_ahead_q90_test], axis=1)\n",
    "    df_ensemble_quantile10 = pd.concat([df_train_ensemble_quantile10, df_test_ensemble_quantile10], axis=0)\n",
    "    df_ensemble_quantile90 = pd.concat([df_train_ensemble_quantile90, df_test_ensemble_quantile90], axis=0)\n",
    "else:\n",
    "    df_train_ensemble_quantile10, df_test_ensemble_quantile10 = None, None\n",
    "    df_train_ensemble_quantile90, df_test_ensemble_quantile90 = None, None\n",
    "    df_ensemble_quantile10, df_ensemble_quantile90 = None, None\n",
    "\n",
    "# normalize dataframe\n",
    "logger.info('   ')\n",
    "logger.opt(colors=True).info(f'<fg 250,128,114> Normalize DataFrame </fg 250,128,114>')\n",
    "df_ensemble_normalized = normalize_dataframe(df_ensemble, maximum_capacity)\n",
    "if ens_params['add_quantile_predictions']:  # normalize quantile predictions\n",
    "    logger.opt(colors=True).info(f'<fg 250,128,114> -- add quantile predictions </fg 250,128,114>')\n",
    "    df_ensemble_normalized_quantile10 = normalize_dataframe(df_ensemble_quantile10, maximum_capacity)\n",
    "    df_ensemble_normalized_quantile90 = normalize_dataframe(df_ensemble_quantile90, maximum_capacity)\n",
    "else:\n",
    "    df_ensemble_normalized_quantile10, df_ensemble_normalized_quantile90 = None, None\n",
    "\n",
    "# augment dataframe\n",
    "logger.info('   ')\n",
    "logger.opt(colors=True).info(f'<fg 250,128,114> Augment DataFrame </fg 250,128,114>')\n",
    "df_ensemble_normalized_lag = create_augmented_dataframe(df_ensemble_normalized, \n",
    "                                                        max_lags=ens_params['max_lags'], \n",
    "                                                        forecasters_diversity=ens_params['forecasters_diversity'], \n",
    "                                                        lagged=ens_params['lagged'], \n",
    "                                                        augmented=ens_params['augment'],\n",
    "                                                        differenciate=ens_params['differenciate'])\n",
    "\n",
    "if ens_params['add_quantile_predictions']:  # augment quantile predictions\n",
    "    logger.opt(colors=True).info(f'<fg 250,128,114> -- augment quantile predictions </fg 250,128,114>')\n",
    "\n",
    "    # augment quantile10 predictions\n",
    "    df_ensemble_normalized_lag_quantile10 = create_augmented_dataframe(df_ensemble_normalized_quantile10,\n",
    "                                                                        max_lags=ens_params['max_lags'], \n",
    "                                                                        forecasters_diversity=ens_params['forecasters_diversity'], \n",
    "                                                                        lagged=ens_params['lagged'], \n",
    "                                                                        augmented=ens_params['augment'], differenciate=ens_params['differenciate'])\n",
    "    # augment quantile90 predictions\n",
    "    df_ensemble_normalized_lag_quantile90 = create_augmented_dataframe(df_ensemble_normalized_quantile90, \n",
    "                                                                    max_lags=ens_params['max_lags'], \n",
    "                                                                    forecasters_diversity=ens_params['forecasters_diversity'], \n",
    "                                                                    lagged=ens_params['lagged'], \n",
    "                                                                    augmented=ens_params['augment'], \n",
    "                                                                    differenciate=ens_params['differenciate'])\n",
    "else:\n",
    "    df_ensemble_normalized_lag_quantile10, df_ensemble_normalized_lag_quantile90 = None, None\n",
    "\n",
    "# concatenate train and test dataframes\n",
    "df_process = pd.concat([df_train, df_test], axis=0)\n",
    "\n",
    "# normalize dataframe\n",
    "df_process_norm = normalize_dataframe(df_process, maximum_capacity)\n",
    "\n",
    "# differenciate dataframe\n",
    "df_process_norm_diff = df_process_norm.copy()\n",
    "lst_cols_diff = ['diff_' + name for name in list(df_process_norm.columns)]\n",
    "df_process_norm_diff.columns = lst_cols_diff\n",
    "\n",
    "df_train_norm_diff, df_test_norm_diff = df_process_norm_diff[df_process_norm_diff.index < start_prediction_timestamp], df_process_norm_diff[df_process_norm_diff.index >= start_prediction_timestamp]\n",
    "df_train_ensemble, df_test_ensemble = prepare_train_test_data(df_ensemble_normalized_lag, df_train_norm_diff, df_test_norm_diff, start_prediction_timestamp, ens_params['max_lags'])\n",
    "\n",
    "if ens_params['add_quantile_predictions']:\n",
    "    df_train_ensemble_quantile10, df_test_ensemble_quantile10 = df_ensemble_normalized_lag_quantile10[df_ensemble_normalized_lag_quantile10.index< start_prediction_timestamp], df_ensemble_normalized_lag_quantile10[df_ensemble_normalized_lag_quantile10.index>= start_prediction_timestamp]\n",
    "    df_train_ensemble_quantile90, df_test_ensemble_quantile90 = df_ensemble_normalized_lag_quantile90[df_ensemble_normalized_lag_quantile90.index< start_prediction_timestamp], df_ensemble_normalized_lag_quantile90[df_ensemble_normalized_lag_quantile90.index>= start_prediction_timestamp]\n",
    "else:\n",
    "    df_train_ensemble_quantile10, df_test_ensemble_quantile10 = None, None\n",
    "    df_train_ensemble_quantile90, df_test_ensemble_quantile90 = None, None\n",
    "\n",
    "df_dayahead = df_test_norm_diff[['diff_norm_dayaheadforecast', 'diff_norm_dayaheadconfidence10', 'diff_norm_dayaheadconfidence90', 'diff_norm_measured']]\n",
    "df_dayahead11h = df_test_norm_diff[['diff_norm_dayahead11hforecast', 'diff_norm_dayahead11hconfidence10', 'diff_norm_dayahead11hconfidence90', 'diff_norm_measured']]\n",
    "df_weekahead = df_test_norm_diff[['diff_norm_weekaheadforecast', 'diff_norm_weekaheadconfidence10', 'diff_norm_weekaheadconfidence90', 'diff_norm_measured']]\n",
    "\n",
    "# assert df_test match df_ensemble_test\n",
    "assert (df_test_norm_diff['diff_norm_measured'] == df_test_ensemble['diff_norm_targ']).all()\n",
    "\n",
    "# make X-y train and test sets\n",
    "X_train, y_train, X_test, y_test = get_numpy_Xy_train_test(df_train_ensemble, df_test_ensemble)\n",
    "\n",
    "if ens_params['add_quantile_predictions']:\n",
    "    X_train_quantile10, X_test_quantile10 = df_train_ensemble_quantile10.values, df_test_ensemble_quantile10.values\n",
    "    X_train_quantile90, X_test_quantile90 = df_train_ensemble_quantile90.values, df_test_ensemble_quantile90.values\n",
    "else:\n",
    "    X_train_quantile10, X_test_quantile10 = None, None\n",
    "    X_train_quantile90, X_test_quantile90 = None, None\n",
    "\n",
    "# assert do not have nans (should do it before in processing file)\n",
    "assert df_train_ensemble.isna().sum().sum() == 0\n",
    "\n",
    "# run ensemble learning\n",
    "logger.info('   ')\n",
    "logger.opt(colors=True).info(f'<fg 250,128,114> Compute Ensemble Predictions </fg 250,128,114>')\n",
    "predictions = {}\n",
    "if i==0:\n",
    "    best_results = {}\n",
    "\n",
    "for quantile in tqdm(ens_params['quantiles'], desc='Quantile Regression'):\n",
    "\n",
    "    # run ensemble predictions\n",
    "    predictions, best_results, fitted_model, X_train_augmented, X_test_augmented, df_train_ensemble_augmented = run_ensemble_predictions_per_quantile(abs_differenciate=ens_params['compute_abs_difference'], \n",
    "                                                                                                                                                        X_train=X_train, X_test=X_test, y_train=y_train, df_train_ensemble=df_train_ensemble, \n",
    "                                                                                                                                                        predictions=predictions, quantile=quantile, add_quantiles=ens_params['add_quantile_predictions'], \n",
    "                                                                                                                                                        augment_q50=ens_params['augment_q50'], nr_cv_splits=ens_params['nr_cv_splits'], model_type=ens_params['model_type'], solver=solver, \n",
    "                                                                                                                                                        gbr_update_every_days=ens_params['gbr_update_every_days'], gbr_config_params=ens_params['gbr_config_params'], \n",
    "                                                                                                                                                        lr_config_params=ens_params['lr_config_params'], plot_importance_gbr=ens_params['plot_importance_gbr'], \n",
    "                                                                                                                                                        best_results=best_results, iteration=i,\n",
    "                                                                                                                                                        X_train_quantile10=X_train_quantile10, X_test_quantile10=X_test_quantile10, df_train_ensemble_quantile10=df_train_ensemble_quantile10, \n",
    "                                                                                                                                                        X_train_quantile90=X_train_quantile90, X_test_quantile90=X_test_quantile90, df_train_ensemble_quantile90=df_train_ensemble_quantile90)\n",
    "    # collect results for forecasting contribution assessment first stage\n",
    "    previous_day_results_first_stage[quantile] = {\"fitted_model\" : fitted_model, \n",
    "                                                \"X_train_augmented\" : X_train_augmented, \n",
    "                                                \"X_test_augmented\" : X_test_augmented, \n",
    "                                                \"df_train_ensemble_augmented\" : df_train_ensemble_augmented}\n",
    "\n",
    "    if ens_params['compute_second_stage'] and quantile == 0.5:\n",
    "\n",
    "        logger.info('   ')\n",
    "        logger.opt(colors=True).info(f'<fg 72,201,176> Compute Variability Predictions </fg 72,201,176>')\n",
    "        # make predictions for variability\n",
    "        predictions_insample = fitted_model.predict(X_train_augmented)\n",
    "        predictions_outsample = fitted_model.predict(X_test_augmented)\n",
    "        # create 2-stage dataframe\n",
    "        df_2stage = create_2stage_dataframe(df_train_ensemble, df_test_ensemble, y_train, y_test, predictions_insample, predictions_outsample)\n",
    "        # differenciate dataframe with lags\n",
    "        df_2stage_process = create_augmented_dataframe_2stage(df_2stage, \n",
    "                                                            ens_params['order_diff'], \n",
    "                                                            max_lags=ens_params['max_lags_var'], \n",
    "                                                            augment=ens_params['augment_var'])\n",
    "        # split train and test\n",
    "        df_2stage_train, df_2stage_test = df_2stage_process[df_2stage_process.index < start_prediction_timestamp], df_2stage_process[df_2stage_process.index >= start_prediction_timestamp]\n",
    "        # make X-y train and test sets for 2-stage model\n",
    "        X_train_2stage, y_train_2stage, X_test_2stage, y_test_2stage = df_2stage_train.drop(columns=['targets']).values, df_2stage_train['targets'].values, df_2stage_test.drop(columns=['targets']).values, df_2stage_test['targets'].values\n",
    "        \n",
    "        \n",
    "        # run ensemble learning for variability\n",
    "        variability_predictions = {}\n",
    "        if i == 0:\n",
    "            best_results_var = {}\n",
    "        \n",
    "        for quantile in tqdm(ens_params['quantiles'], desc='Quantile Regression'):\n",
    "\n",
    "            # run ensemble predictions for variability\n",
    "            variability_predictions, best_results_var, var_fitted_model = run_ensemble_variability_predictions(X_train_2stage = X_train_2stage, y_train_2stage=y_train_2stage, X_test_2stage=X_test_2stage,\n",
    "                                                                                                            variability_predictions=variability_predictions, \n",
    "                                                                                                            quantile=quantile, nr_cv_splits=ens_params['nr_cv_splits'], \n",
    "                                                                                                            var_model_type=ens_params['var_model_type'], \n",
    "                                                                                                            solver=solver, \n",
    "                                                                                                            var_gbr_config_params=ens_params['var_gbr_config_params'], \n",
    "                                                                                                            var_lr_config_params=ens_params['var_lr_config_params'], \n",
    "                                                                                                            gbr_update_every_days=ens_params['gbr_update_every_days'], \n",
    "                                                                                                            iteration=i, best_results_var=best_results_var)\n",
    "            # collect results for forecasting contribution assessment second stage\n",
    "            previous_day_results_second_stage[quantile] = {\"fitted_model\" : fitted_model,\n",
    "                                                        \"var_fitted_model\" : var_fitted_model,\n",
    "                                                        \"X_train_augmented\" : X_train_augmented,\n",
    "                                                        \"X_test_augmented\" : X_test_augmented,\n",
    "                                                        \"df_train_ensemble_augmented\" : df_train_ensemble_augmented,\n",
    "                                                        \"df_train_ensemble\" : df_train_ensemble,\n",
    "                                                        \"df_test_ensemble\" : df_test_ensemble,}\n",
    "            \n",
    "        # collect results as dictionary\n",
    "        var_predictions_dict = collect_quantile_ensemble_predictions(quantiles=ens_params['quantiles'], \n",
    "                                                                    test_data=df_2stage_test, predictions=variability_predictions)\n",
    "        # create dataframe for variability ensemble\n",
    "        df_var_ensemble = create_var_ensemble_dataframe(quantiles=ens_params['quantiles'], \n",
    "                                                        quantile_predictions_dict=var_predictions_dict, df_test=df_2stage_test)\n",
    "        \n",
    "        # delete variables\n",
    "        del df_2stage, df_2stage_process, df_2stage_train\n",
    "        gc.collect()\n",
    "\n",
    "    # delete variables\n",
    "    del X_train_augmented, X_test_augmented, df_train_ensemble_augmented\n",
    "    gc.collect()\n",
    "\n",
    "# collect results as dictionary\n",
    "quantile_predictions_dict = collect_quantile_ensemble_predictions(quantiles=ens_params['quantiles'], \n",
    "                                                                test_data=df_test_norm_diff, predictions=predictions)\n",
    "\n",
    "# collect results as dataframe\n",
    "df_pred_ensemble = create_ensemble_dataframe(ens_params['quantiles'],\n",
    "                                                quantile_predictions_dict, df_test_norm_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wind power forecast - 1st stage\n",
    "\n",
    "#----------------- Forecasters -----------------#\n",
    "# ensemble predictions\n",
    "df_pred_ensemble = create_ensemble_dataframe(ens_params['quantiles'],\n",
    "                                            quantile_predictions_dict, df_test_norm_diff)\n",
    "# weighted average predictions\n",
    "df_weighted_avg, dict_weights = calculate_weighted_avg(df_train_norm_diff, df_test_norm_diff, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'])\n",
    "# equal weights predictions\n",
    "df_equal_weights = calculate_equal_weights(df_test_norm_diff)\n",
    "# day-ahead forecast predictions\n",
    "df_dayahead = df_test_norm_diff[['diff_norm_dayaheadforecast', 'diff_norm_dayaheadconfidence10', 'diff_norm_dayaheadconfidence90', 'diff_norm_measured']]\n",
    "# day-ahead-11h forecast predictions\n",
    "df_dayahead_11h = df_test_norm_diff[['diff_norm_dayahead11hforecast', 'diff_norm_dayahead11hconfidence10', 'diff_norm_dayahead11hconfidence90', 'diff_norm_measured']]\n",
    "# week-ahead forecast predictions\n",
    "df_week_ahead = df_test_norm_diff[['diff_norm_weekaheadforecast', 'diff_norm_weekaheadconfidence10', 'diff_norm_weekaheadconfidence90', 'diff_norm_measured']]\n",
    "#----------------- Performance Metrics -----------------#\n",
    "# performance ensemble\n",
    "rmse_ensemble = round(calculate_rmse(df_pred_ensemble, '50_predictions').values[0][0], 3)\n",
    "lst_rmse_gbr_ensemble.append(rmse_ensemble)\n",
    "pinball_ensemble = calculate_pinball_losses(df_pred_ensemble, '10_predictions', '90_predictions')\n",
    "pinball_ensemble_q10 = round(pinball_ensemble['pb_loss_10'].values[0], 3)\n",
    "pinball_ensemble_q90 = round(pinball_ensemble['pb_loss_90'].values[0], 3)\n",
    "lst_pb_gbr_ensemble_q10.append(pinball_ensemble_q10)\n",
    "lst_pb_gbr_ensemble_q90.append(pinball_ensemble_q90)\n",
    "# performance weighted average\n",
    "rmse_weighted_avg = round(calculate_rmse(df_weighted_avg, 'mean_prediction').values[0][0], 3)\n",
    "lst_rmse_weighted_avg.append(rmse_weighted_avg)\n",
    "pinball_weighted_avg = calculate_pinball_losses(df_weighted_avg, 'Q10', 'Q90')\n",
    "pinball_weighted_avg_q10 = round(pinball_weighted_avg['pb_loss_10'].values[0], 3)\n",
    "pinball_weighted_avg_q90 = round(pinball_weighted_avg['pb_loss_90'].values[0], 3)\n",
    "lst_pb_weighted_avg_q10.append(pinball_weighted_avg_q10)\n",
    "lst_pb_weighted_avg_q90.append(pinball_weighted_avg_q90)\n",
    "# performance equal weights\n",
    "rmse_equal_weights = round(calculate_rmse(df_equal_weights, 'mean_prediction').values[0][0], 3)\n",
    "lst_rmse_equal_weights.append(rmse_equal_weights)\n",
    "pinball_equal_weights = calculate_pinball_losses(df_equal_weights, 'Q10', 'Q90')\n",
    "pinball_equal_weights_q10 = round(pinball_equal_weights['pb_loss_10'].values[0], 3)\n",
    "pinball_equal_weights_q90 = round(pinball_equal_weights['pb_loss_90'].values[0], 3)\n",
    "lst_pb_equal_weights_q10.append(pinball_equal_weights_q10)\n",
    "lst_pb_equal_weights_q90.append(pinball_equal_weights_q90)\n",
    "# performance day-ahead\n",
    "rmse_dayahead = round(calculate_rmse(df_dayahead, 'diff_norm_dayaheadforecast').values[0][0], 3)\n",
    "lst_rmse_baseline_dayahead.append(rmse_dayahead)\n",
    "pinball_dayahead = calculate_pinball_losses(df_dayahead, 'diff_norm_dayaheadconfidence10', 'diff_norm_dayaheadconfidence90')\n",
    "pinball_dayahead_q10 = round(pinball_dayahead['pb_loss_10'].values[0], 3)\n",
    "pinball_dayahead_q90 = round(pinball_dayahead['pb_loss_90'].values[0], 3)\n",
    "lst_pb_dayahead_q10.append(pinball_dayahead_q10)\n",
    "lst_pb_dayahead_q90.append(pinball_dayahead_q90)\n",
    "# performance day-ahead-11h\n",
    "rmse_dayahead_11h = round(calculate_rmse(df_dayahead_11h, 'diff_norm_dayahead11hforecast').values[0][0], 3)\n",
    "lst_rmse_baseline_dayahead11h.append(rmse_dayahead_11h)\n",
    "pinball_dayahead_11h = calculate_pinball_losses(df_dayahead_11h, 'diff_norm_dayahead11hconfidence10', 'diff_norm_dayahead11hconfidence90')\n",
    "pinball_dayahead_11h_q10 = round(pinball_dayahead_11h['pb_loss_10'].values[0], 3)\n",
    "pinball_dayahead_11h_q90 = round(pinball_dayahead_11h['pb_loss_90'].values[0], 3)\n",
    "lst_pb_dayahead_11h_q10.append(pinball_dayahead_11h_q10)\n",
    "lst_pb_dayahead_11h_q90.append(pinball_dayahead_11h_q90)\n",
    "# performance week ahead\n",
    "rmse_week_ahead = round(calculate_rmse(df_week_ahead, 'diff_norm_weekaheadforecast').values[0][0], 3)\n",
    "lst_rmse_baseline_week_ahead.append(rmse_week_ahead)\n",
    "pinball_week_ahead = calculate_pinball_losses(df_week_ahead, 'diff_norm_weekaheadconfidence10', 'diff_norm_weekaheadconfidence90')\n",
    "pinball_week_ahead_q10 = round(pinball_week_ahead['pb_loss_10'].values[0], 3)\n",
    "pinball_week_ahead_q90 = round(pinball_week_ahead['pb_loss_90'].values[0], 3)\n",
    "lst_pb_week_ahead_q10.append(pinball_week_ahead_q10)\n",
    "lst_pb_week_ahead_q90.append(pinball_week_ahead_q90)\n",
    "\n",
    "\n",
    "# Variability wind power forecast - 2nd stage\n",
    "# concatenate last training row with test data\n",
    "df_test_norm_var = pd.concat([df_train_norm_diff.iloc[-1:, :], df_test_norm_diff], axis=0).diff().iloc[1:, :]\n",
    "#----------------- Forecasters -----------------#\n",
    "# equal weights predictions\n",
    "df_equal_weights_var = calculate_equal_weights(df_test_norm_var)\n",
    "# weighted average predictions\n",
    "df_weighted_avg_var, dict_weights_var = calculate_weighted_avg(df_train_norm_diff, df_test_norm_diff, start_prediction_timestamp, window_size_valid=weight_avg_params['window_size_valid'], var=True)\n",
    "# day-ahead forecast predictions\n",
    "df_dayahead_var = df_test_norm_var[['diff_norm_dayaheadforecast', 'diff_norm_measured']]\n",
    "# day-ahead-11h forecast predictions\n",
    "df_dayahead_11h_var = df_test_norm_var[['diff_norm_dayahead11hforecast', 'diff_norm_measured']]\n",
    "# week-ahead forecast predictions\n",
    "df_week_ahead_var = df_test_norm_var[['diff_norm_weekaheadforecast', 'diff_norm_measured']]\n",
    "#----------------- Performance Metrics -----------------#\n",
    "# performance variability ensemble\n",
    "rmse_var_ensemble = round(calculate_rmse(df_var_ensemble, '50_var_predictions', targ_col='targets').values[0][0], 3)\n",
    "lst_rmse_var_gbr_ensemble.append(rmse_var_ensemble)\n",
    "# performance weighted average\n",
    "rmse_var_weighted_avg = round(calculate_rmse(df_weighted_avg_var, 'mean_prediction').values[0][0], 3)\n",
    "lst_rmse_var_weighted_avg.append(rmse_var_weighted_avg)\n",
    "# performance equal weights\n",
    "rmse_var_equal_weights = round(calculate_rmse(df_equal_weights_var, 'mean_prediction').values[0][0], 3)\n",
    "lst_rmse_var_equal_weights.append(rmse_var_equal_weights)\n",
    "# performance day-ahead\n",
    "rmse_var_dayahead = round(calculate_rmse(df_dayahead_var, 'diff_norm_dayaheadforecast').values[0][0], 3)\n",
    "lst_rmse_var_baseline_dayahead.append(rmse_var_dayahead)\n",
    "# performance day-ahead-11h\n",
    "rmse_var_dayahead_11h = round(calculate_rmse(df_dayahead_11h_var, 'diff_norm_dayahead11hforecast').values[0][0], 3)\n",
    "lst_rmse_var_baseline_dayahead11h.append(rmse_var_dayahead_11h)\n",
    "# performance week ahead\n",
    "rmse_var_week_ahead = round(calculate_rmse(df_week_ahead_var, 'diff_norm_weekaheadforecast').values[0][0], 3)\n",
    "lst_rmse_var_baseline_week_ahead.append(rmse_var_week_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------- Display Results -----------------#\n",
    "#\n",
    "if ens_params['plot_baseline']:\n",
    "    plot_baseline_forecasts(df_dayahead, model_name='Day-Ahead Model')\n",
    "    plot_baseline_forecasts(df_dayahead11h, model_name='Day-Ahead-11h Model')\n",
    "    plot_baseline_forecasts(df_weekahead, model_name='Week-Ahead Model')\n",
    "\n",
    "# plot forecasts weighted avg\n",
    "if ens_params['plot_weighted_avg']:\n",
    "    plot_weighted_avg_forecasts(df_weighted_avg)\n",
    "\n",
    "# plot forecasts ensemble\n",
    "plot_ensemble_forecasts(df_pred_ensemble, df_test_ensemble)\n",
    "nr_previous_days = len(pd.date_range(start=start_training_timestamp, end=end_training_timestamp, freq='1D')) - 1\n",
    "plt.title(f'Ensemble Forecasts - Quantile {ens_params[\"model_type\"]}')\n",
    "plot_ramp_events(df_test_norm_diff, ens_params['compute_abs_difference'])\n",
    "if not ens_params['zoom_in_variability']:  # zoom in the variability forecasts\n",
    "    plt.ylim(-0.01, 1)\n",
    "plt.show()\n",
    "\n",
    "# plot variability forecast results\n",
    "plot_var_ensemble_forecasts(df_var_ensemble, df_2stage_test)\n",
    "nr_previous_days = len(pd.date_range(start=start_training_timestamp, end=end_training_timestamp, freq='1D')) - 1\n",
    "plt.title(f'Ensemble Variability Forecasts - Quantile {ens_params[\"var_model_type\"]}')\n",
    "plot_ramp_events(df_test_norm_diff, ens_params['compute_abs_difference'])\n",
    "if not ens_params['zoom_in_variability']:  # zoom in the variability forecasts\n",
    "    plt.ylim(-0.6, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasters Contribution Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot contribution weighted average\n",
    "if ens_params['plot_importance_weighted_avg']:\n",
    "    for quantile in ens_params['quantiles']:\n",
    "        plot_weight_avg_contributions(dict_weights, quantile, stage='Wind Power', days = weight_avg_params['window_size_valid'])\n",
    "\n",
    "# plot contribution weighted average\n",
    "if ens_params['plot_importance_weighted_avg']:\n",
    "    plot_weight_avg_contributions(dict_weights_var, quantile=0.5, stage='Wind Power Variability', days= weight_avg_params['window_size_valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sim_params['display_metrics']:\n",
    "    display_forecasting_metrics(ens_params=ens_params, \n",
    "                                rmse_ensemble=rmse_ensemble, rmse_weighted_avg=rmse_weighted_avg, rmse_equal_weights=rmse_equal_weights,\n",
    "                                rmse_dayahead=rmse_dayahead, rmse_dayahead_11h=rmse_dayahead_11h, rmse_week_ahead=rmse_week_ahead,\n",
    "                                pinball_ensemble_q10=pinball_ensemble_q10, pinball_weighted_avg_q10=pinball_weighted_avg_q10, pinball_equal_weights_q10=pinball_equal_weights_q10,\n",
    "                                pinball_dayahead_q10=pinball_dayahead_q10, pinball_dayahead_11h_q10=pinball_dayahead_11h_q10, pinball_week_ahead_q10=pinball_week_ahead_q10,\n",
    "                                pinball_ensemble_q90=pinball_ensemble_q90, pinball_weighted_avg_q90=pinball_weighted_avg_q90, pinball_equal_weights_q90=pinball_equal_weights_q90,\n",
    "                                pinball_dayahead_q90=pinball_dayahead_q90, pinball_dayahead_11h_q90=pinball_dayahead_11h_q90, pinball_week_ahead_q90=pinball_week_ahead_q90,\n",
    "                                rmse_var_ensemble=rmse_var_ensemble, rmse_var_weighted_avg=rmse_var_weighted_avg, rmse_var_equal_weights=rmse_var_equal_weights,\n",
    "                                rmse_var_dayahead=rmse_var_dayahead, rmse_var_dayahead_11h=rmse_var_dayahead_11h, rmse_var_week_ahead=rmse_var_week_ahead)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
